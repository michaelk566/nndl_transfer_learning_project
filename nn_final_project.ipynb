{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Persist data on google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D0Rt37qm3WP",
        "outputId": "162f73a9-ed5e-43e7-bcae-b897dfbdf5ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMS_4995/final_project/\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "NwjLsd9zpLJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a862565-8ff9-4cdc-cd04-786f90dd86b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-c7q2uf4v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-c7q2uf4v\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Required Data"
      ],
      "metadata": {
        "id": "qE9OjQbwIQ2q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9rV7SF0lDp4",
        "outputId": "6f82be97-9bc8-4fba-fee8-4325955ba434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_images/ folder already exists. Skipping unzip.\n",
            "test_images/ folder already exists. Skipping unzip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/COMS_4995/final_project/'\n",
        "image_dir = base_dir + 'data/train_images/'\n",
        "zip_path = base_dir + \"data/train_images.zip\"\n",
        "\n",
        "train_zip_path = base_dir + \"data/train_images.zip\"\n",
        "train_extract_dir = base_dir + \"data/\"\n",
        "test_zip_path = base_dir + \"data/test_images.zip\"\n",
        "test_extract_dir = base_dir + \"data/\"\n",
        "\n",
        "# Used AI here to write boidler plate unzipping code that unzips if its not already unzipped\n",
        "\n",
        "# === Unzip train_images if not already extracted ===\n",
        "if not os.path.exists(train_extract_dir):\n",
        "    print(\"Unzipping train_images.zip...\")\n",
        "    with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(train_extract_dir)\n",
        "    print(\"Unzipped to:\", train_extract_dir)\n",
        "else:\n",
        "    print(\"train_images/ folder already exists. Skipping unzip.\")\n",
        "\n",
        "# === Unzip test_images if not already extracted ===\n",
        "if not os.path.exists(test_extract_dir):\n",
        "    print(\"Unzipping test_images.zip...\")\n",
        "    with zipfile.ZipFile(test_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(test_extract_dir)\n",
        "    print(\"Unzipped to:\", test_extract_dir)\n",
        "else:\n",
        "    print(\"test_images/ folder already exists. Skipping unzip.\")\n",
        "\n",
        "# Load CSVs\n",
        "train_df = pd.read_csv(base_dir + \"data/train_data.csv\")\n",
        "superclass_map = pd.read_csv(base_dir + \"data/superclass_mapping.csv\")\n",
        "subclass_map = pd.read_csv(base_dir + \"data/subclass_mapping.csv\")\n",
        "\n",
        "# Merge labels for readability\n",
        "train_df = train_df.merge(superclass_map, left_on=\"superclass_index\", right_index=True, suffixes=('', '_super'))\n",
        "train_df = train_df.merge(subclass_map, left_on=\"subclass_index\", right_index=True, suffixes=('', '_sub'))\n",
        "\n",
        "# Rename for clarity\n",
        "train_df.rename(columns={\"class\": \"superclass_name\", \"class_sub\": \"subclass_name\"}, inplace=True)\n",
        "train_df = train_df.drop(columns=[\"index\", \"index_sub\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore/Visualize Training Data"
      ],
      "metadata": {
        "id": "iI6mcVe1mpRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Used AI here to automate the print statements below, but the stats logic was written by us\n",
        "\n",
        "total_images = len(train_df)\n",
        "print(\"Total training images:\", total_images)\n",
        "\n",
        "num_superclasses = train_df['superclass_name'].nunique()\n",
        "num_subclasses = train_df['subclass_name'].nunique()\n",
        "print(\"Unique superclasses:\", num_superclasses)\n",
        "print(\"Unique subclasses:\", num_subclasses)\n",
        "\n",
        "# === Superclass Counts with Percentages ===\n",
        "superclass_counts = train_df['superclass_name'].value_counts().reset_index()\n",
        "superclass_counts.columns = ['Superclass', 'Image Count']\n",
        "superclass_counts['Percent of Total'] = (superclass_counts['Image Count'] / total_images * 100).round(2)\n",
        "\n",
        "print(\"\\nImages per Superclass:\")\n",
        "print(superclass_counts)\n",
        "\n",
        "# === Group by (superclass, subclass) ===\n",
        "grouped = train_df.groupby(['superclass_name', 'subclass_name']).size().reset_index(name='Image Count')\n",
        "\n",
        "# Total per superclass (for per-superclass percentages)\n",
        "super_totals = train_df.groupby('superclass_name').size().to_dict()\n",
        "total_images = len(train_df)\n",
        "\n",
        "# Compute percentages\n",
        "grouped['% of Superclass'] = grouped.apply(\n",
        "    lambda row: (row['Image Count'] / super_totals[row['superclass_name']] * 100), axis=1\n",
        ").round(2)\n",
        "\n",
        "grouped['% of Total'] = (grouped['Image Count'] / total_images * 100).round(2)\n",
        "\n",
        "# Sort and display\n",
        "grouped = grouped.sort_values(by=['superclass_name', 'Image Count'], ascending=[True, False])\n",
        "\n",
        "# === Global (across all data) stats ===\n",
        "global_percents = grouped['% of Total'].values\n",
        "print(\"\\n Stats on % of Total Dataset per (superclass, subclass):\")\n",
        "print(f\"  Min:    {global_percents.min():.2f}%\")\n",
        "print(f\"  Max:    {global_percents.max():.2f}%\")\n",
        "print(f\"  Mean:   {global_percents.mean():.2f}%\")\n",
        "print(f\"  Median: {np.median(global_percents):.2f}%\")\n",
        "print(f\"  Std:    {global_percents.std():.2f}%\")\n",
        "\n",
        "# === Per-superclass relative stats ===\n",
        "relative_percents = grouped['% of Superclass'].values\n",
        "print(\"\\n Stats on % of Superclass per subclass:\")\n",
        "print(f\"  Min:    {relative_percents.min():.2f}%\")\n",
        "print(f\"  Max:    {relative_percents.max():.2f}%\")\n",
        "print(f\"  Mean:   {relative_percents.mean():.2f}%\")\n",
        "print(f\"  Median: {np.median(relative_percents):.2f}%\")\n",
        "print(f\"  Std:    {relative_percents.std():.2f}%\")\n",
        "\n",
        "\n",
        "superclasses = grouped['superclass_name'].unique()\n",
        "\n",
        "for sc in superclasses:\n",
        "    print(f\"\\n Stats for subclasses within superclass: {sc}\")\n",
        "\n",
        "    subset = grouped[grouped['superclass_name'] == sc]['% of Superclass'].values\n",
        "    print(f\"  Number of subclasses: {len(subset)}\")\n",
        "    print(f\"  Min:    {subset.min():.2f}%\")\n",
        "    print(f\"  Max:    {subset.max():.2f}%\")\n",
        "    print(f\"  Mean:   {subset.mean():.2f}%\")\n",
        "    print(f\"  Median: {np.median(subset):.2f}%\")\n",
        "    print(f\"  Std:    {subset.std():.2f}%\")\n",
        "\n",
        "max_count = grouped['Image Count'].max()\n",
        "min_count = grouped['Image Count'].min()\n",
        "\n",
        "print(\"\\nSubclass with most images:\")\n",
        "print(f\"  {max_count} images\")\n",
        "\n",
        "print(\"Subclass with fewest images:\")\n",
        "print(f\"  {min_count} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-ExWseSIOzC",
        "outputId": "b6ea1112-bd73-452d-f26f-3d5456b3720e"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training images: 6288\n",
            "Unique superclasses: 3\n",
            "Unique subclasses: 87\n",
            "\n",
            "Images per Superclass:\n",
            "  Superclass  Image Count  Percent of Total\n",
            "0    reptile         2354             37.44\n",
            "1        dog         2084             33.14\n",
            "2       bird         1850             29.42\n",
            "\n",
            " Stats on % of Total Dataset per (superclass, subclass):\n",
            "  Min:    0.78%\n",
            "  Max:    1.62%\n",
            "  Mean:   1.15%\n",
            "  Median: 0.80%\n",
            "  Std:    0.40%\n",
            "\n",
            " Stats on % of Superclass per subclass:\n",
            "  Min:    2.08%\n",
            "  Max:    5.41%\n",
            "  Mean:   3.45%\n",
            "  Median: 2.70%\n",
            "  Std:    1.16%\n",
            "\n",
            " Stats for subclasses within superclass: bird\n",
            "  Number of subclasses: 29\n",
            "  Min:    2.70%\n",
            "  Max:    5.41%\n",
            "  Mean:   3.45%\n",
            "  Median: 2.70%\n",
            "  Std:    1.21%\n",
            "\n",
            " Stats for subclasses within superclass: dog\n",
            "  Number of subclasses: 29\n",
            "  Min:    2.35%\n",
            "  Max:    4.80%\n",
            "  Mean:   3.45%\n",
            "  Median: 2.35%\n",
            "  Std:    1.22%\n",
            "\n",
            " Stats for subclasses within superclass: reptile\n",
            "  Number of subclasses: 29\n",
            "  Min:    2.08%\n",
            "  Max:    4.33%\n",
            "  Mean:   3.45%\n",
            "  Median: 4.25%\n",
            "  Std:    1.06%\n",
            "\n",
            "Subclass with most images:\n",
            "  102 images\n",
            "Subclass with fewest images:\n",
            "  49 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "caKLk2pKqqoN",
        "outputId": "97dd9346-689f-4334-a6f2-5bd4eda03a17"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image  superclass_index  subclass_index  \\\n",
              "0  0.jpg                 1              37   \n",
              "1  1.jpg                 0              42   \n",
              "2  2.jpg                 1              62   \n",
              "3  3.jpg                 1              31   \n",
              "4  4.jpg                 0               4   \n",
              "\n",
              "                                         description superclass_name  \\\n",
              "0  nature photograph of a dog, specifically a Mal...             dog   \n",
              "1  nature photograph of a bird, specifically a oy...            bird   \n",
              "2  nature photograph of a dog, specifically a Afg...             dog   \n",
              "3  nature photograph of a dog, specifically a Shi...             dog   \n",
              "4  nature photograph of a bird, specifically a gr...            bird   \n",
              "\n",
              "                                    subclass_name  \n",
              "0           Maltese dog, Maltese terrier, Maltese  \n",
              "1                   oystercatcher, oyster catcher  \n",
              "2                            Afghan hound, Afghan  \n",
              "3                                        Shih-Tzu  \n",
              "4  great grey owl, great gray owl, Strix nebulosa  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb26177e-81fb-44f7-9a7b-8231a21f85ca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>superclass_index</th>\n",
              "      <th>subclass_index</th>\n",
              "      <th>description</th>\n",
              "      <th>superclass_name</th>\n",
              "      <th>subclass_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>nature photograph of a dog, specifically a Mal...</td>\n",
              "      <td>dog</td>\n",
              "      <td>Maltese dog, Maltese terrier, Maltese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>nature photograph of a bird, specifically a oy...</td>\n",
              "      <td>bird</td>\n",
              "      <td>oystercatcher, oyster catcher</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>62</td>\n",
              "      <td>nature photograph of a dog, specifically a Afg...</td>\n",
              "      <td>dog</td>\n",
              "      <td>Afghan hound, Afghan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>nature photograph of a dog, specifically a Shi...</td>\n",
              "      <td>dog</td>\n",
              "      <td>Shih-Tzu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>nature photograph of a bird, specifically a gr...</td>\n",
              "      <td>bird</td>\n",
              "      <td>great grey owl, great gray owl, Strix nebulosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb26177e-81fb-44f7-9a7b-8231a21f85ca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cb26177e-81fb-44f7-9a7b-8231a21f85ca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cb26177e-81fb-44f7-9a7b-8231a21f85ca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8e6c1578-cd61-45dd-9c4e-a53d0f8668a9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8e6c1578-cd61-45dd-9c4e-a53d0f8668a9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8e6c1578-cd61-45dd-9c4e-a53d0f8668a9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 6288,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6288,\n        \"samples\": [\n          \"4441.jpg\",\n          \"3658.jpg\",\n          \"6068.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"superclass_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subclass_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24,\n        \"min\": 0,\n        \"max\": 86,\n        \"num_unique_values\": 87,\n        \"samples\": [\n          12,\n          37,\n          22\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 87,\n        \"samples\": [\n          \"nature photograph of a dog, specifically a Tibetan terrier, chrysanthemum dog, centered composition, photorealistic, 8k, the animal is fully visible within frame and not cut off\",\n          \"nature photograph of a dog, specifically a Maltese dog, Maltese terrier, Maltese, centered composition, photorealistic, 8k, the animal is fully visible within frame and not cut off\",\n          \"nature photograph of a dog, specifically a Japanese spaniel, centered composition, photorealistic, 8k, the animal is fully visible within frame and not cut off\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"superclass_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"dog\",\n          \"bird\",\n          \"reptile\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subclass_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 87,\n        \"samples\": [\n          \"Tibetan terrier, chrysanthemum dog\",\n          \"Maltese dog, Maltese terrier, Maltese\",\n          \"Japanese spaniel\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "mkhF8dh6sUq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code was mainly taken from the template code given to us by teaching staff\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        superclass_label = row['superclass_index']\n",
        "        subclass_label = row['subclass_index']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, superclass_label, subclass_label\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name"
      ],
      "metadata": {
        "id": "QPlhfGcysWsb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Incorporate Tiny ImageNet for example images of novel data"
      ],
      "metadata": {
        "id": "RtJilJ1g5OgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "!pip install -U datasets\n",
        "ds_train = load_dataset(\"slegroux/tiny-imagenet-200-clean\", split=\"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D35f2Lik6Acc",
        "outputId": "d0854234-727a-43f2-e67f-f44d78d42f28"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip tiny-imagenet-200.zip tiny-imagenet-200/words.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH_Edthr9meP",
        "outputId": "7815ef4f-a063-42ac-fd1f-e390b25d882f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2025-05-13 18:42:03--  https://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip.1’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  57.9MB/s    in 4.6s    \n",
            "\n",
            "2025-05-13 18:42:08 (51.2 MB/s) - ‘tiny-imagenet-200.zip.1’ saved [248100043/248100043]\n",
            "\n",
            "Archive:  tiny-imagenet-200.zip\n",
            "replace tiny-imagenet-200/words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grab mapping of id to human readable label\n",
        "wnid_to_label = {}\n",
        "with open(\"tiny-imagenet-200/words.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        wnid, label = line.strip().split(\"\\t\")\n",
        "        wnid_to_label[wnid] = label\n",
        "\n",
        "# get the ids\n",
        "synsets = ds_train.features[\"label\"].names\n",
        "\n",
        "# map id to human readable labels\n",
        "idx_to_label = {i: wnid_to_label[wnid] for i, wnid in enumerate(synsets)}\n",
        "unique_labels = set(label for id, label in idx_to_label.items())\n",
        "\n",
        "print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "print(unique_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHPp0Iqx_NM9",
        "outputId": "7c69246c-b9d1-4d26-d25e-efe48c73f677"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique labels: 200\n",
            "{'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'brown bear, bruin, Ursus arctos', 'oboe, hautboy, hautbois', 'military uniform', 'brain coral', \"potter's wheel\", 'lifeboat', 'steel arch bridge', 'triumphal arch', 'vestment', 'kimono', 'suspension bridge', 'seashore, coast, seacoast, sea-coast', 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus', 'slug', 'cockroach, roach', 'mashed potato', 'fur coat', 'bathtub, bathing tub, bath, tub', 'trilobite', 'frying pan, frypan, skillet', 'gasmask, respirator, gas helmet', 'lawn mower, mower', 'altar', 'sandal', 'confectionery, confectionary, candy store', 'scoreboard', 'banana', 'goldfish, Carassius auratus', \"spider web, spider's web\", 'bison', 'organ, pipe organ', 'alp', 'grasshopper, hopper', 'maypole', 'barn', 'American alligator, Alligator mississipiensis', 'albatross, mollymawk', 'apron', 'obelisk', 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui', 'pretzel', 'meat loaf, meatloaf', 'walking stick, walkingstick, stick insect', 'cauliflower', 'picket fence, paling', 'pizza, pizza pie', 'backpack, back pack, knapsack, packsack, rucksack, haversack', 'CD player', 'bee', 'trolleybus, trolley coach, trackless trolley', 'beacon, lighthouse, beacon light, pharos', 'beaker', 'Arabian camel, dromedary, Camelus dromedarius', 'viaduct', 'sombrero', 'coral reef', 'chimpanzee, chimp, Pan troglodytes', 'miniskirt, mini', 'computer keyboard, keypad', 'lampshade, lamp shade', 'water jug', 'turnstile', 'tarantula', 'sunglasses, dark glasses, shades', 'hourglass', 'orangutan, orang, orangutang, Pongo pygmaeus', 'ice lolly, lolly, lollipop, popsicle', 'pill bottle', 'comic book', 'cardigan', 'space heater', 'plate', 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon', 'lemon', 'sea cucumber, holothurian', 'rocking chair, rocker', 'bell pepper', 'Labrador retriever', 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle', 'pole', 'freight car', 'cannon', 'tractor', 'sports car, sport car', 'volleyball', 'hog, pig, grunter, squealer, Sus scrofa', 'Chihuahua', 'rugby ball', 'baboon', 'lion, king of beasts, Panthera leo', 'flagpole, flagstaff', 'guacamole', 'goose', 'pay-phone, pay-station', 'broom', 'butcher shop, meat market', 'crane', 'parking meter', 'sewing machine', 'punching bag, punch bag, punching ball, punchball', 'European fire salamander, Salamandra salamandra', 'limousine, limo', 'Yorkshire terrier', 'Egyptian cat', 'iPod', 'potpie', 'lakeside, lakeshore', 'refrigerator, icebox', 'drumstick', 'basketball', 'scorpion', 'bullfrog, Rana catesbeiana', 'tabby, tabby cat', 'bullet train, bullet', 'snorkel', 'gazelle', 'cliff, drop, drop-off', 'centipede', 'guinea pig, Cavia cobaya', 'snail', 'jinrikisha, ricksha, rickshaw', 'remote control, remote', 'mantis, mantid', 'sulphur butterfly, sulfur butterfly', 'teddy, teddy bear', 'black widow, Latrodectus mactans', 'syringe', 'dam, dike, dyke', 'chest', 'magnetic compass', 'birdhouse', 'jellyfish', 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis', 'ice cream, icecream', 'neck brace', 'orange', 'barrel, cask', 'sock', 'chain', 'German shepherd, German shepherd dog, German police dog, alsatian', 'swimming trunks, bathing trunks', 'water tower', 'mushroom', 'go-kart', 'nail', 'bannister, banister, balustrade, balusters, handrail', 'projectile, missile', 'thatch, thatched roof', 'bucket, pail', 'wooden spoon', 'black stork, Ciconia nigra', 'ox', 'abacus', 'wok', 'acorn', 'torch', 'school bus', 'cliff dwelling', 'Christmas stocking', 'moving van', 'dining table, board', 'bikini, two-piece', 'binoculars, field glasses, opera glasses', 'Persian cat', 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus', 'desk', 'stopwatch, stop watch', 'umbrella', 'candle, taper, wax light', \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\", 'dumbbell', 'gondola', 'reel', 'king penguin, Aptenodytes patagonica', 'pop bottle, soda bottle', 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish', \"academic gown, academic robe, judge's robe\", \"plunger, plumber's helper\", 'boa constrictor, Constrictor constrictor', 'golden retriever', 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM', 'convertible', 'dugong, Dugong dugon', 'pomegranate', 'sea slug, nudibranch', 'fly', 'barbershop', 'bow tie, bow-tie, bowtie', 'poncho', 'African elephant, Loxodonta africana', 'espresso', 'brass, memorial tablet, plaque', 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria', 'teapot', 'American lobster, Northern lobster, Maine lobster, Homarus americanus', 'standard poodle', 'beer bottle', 'fountain', 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manually extracted novel classes from dataset (gave the list of all labels to AI and told it to filter, then we manually confirmed)\n",
        "non_reptile_bird_dog_labels = {\n",
        "    'thatch, thatched roof',\n",
        "    'parking meter',\n",
        "    'syringe',\n",
        "    'gondola',\n",
        "    'dumbbell',\n",
        "    'altar',\n",
        "    'drumstick',\n",
        "    'centipede',\n",
        "    'cannon',\n",
        "    'limousine, limo',\n",
        "    'stopwatch, stop watch',\n",
        "    'CD player',\n",
        "    'basketball',\n",
        "    'meat loaf, meatloaf',\n",
        "    'chain',\n",
        "    'orangutan, orang, orangutang, Pongo pygmaeus',\n",
        "    'brass, memorial tablet, plaque',\n",
        "    'sunglasses, dark glasses, shades',\n",
        "    'walking stick, walkingstick, stick insect',\n",
        "    'sulphur butterfly, sulfur butterfly',\n",
        "    'sea slug, nudibranch',\n",
        "    'comic book',\n",
        "    'bell pepper',\n",
        "    'pomegranate',\n",
        "    'convertible',\n",
        "    'triumphal arch',\n",
        "    'punching bag, punch bag, punching ball, punchball',\n",
        "    \"spider web, spider's web\",\n",
        "    'miniskirt, mini',\n",
        "    'mushroom',\n",
        "    'frying pan, frypan, skillet',\n",
        "    'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
        "    'bikini, two-piece',\n",
        "    'cockroach, roach',\n",
        "    'sewing machine',\n",
        "    'cliff, drop, drop-off',\n",
        "    'orange',\n",
        "    'military uniform',\n",
        "    'refrigerator, icebox',\n",
        "    'beer bottle',\n",
        "    'cauliflower',\n",
        "    'slug',\n",
        "    'scoreboard',\n",
        "    'poncho',\n",
        "    'desk',\n",
        "    'guacamole',\n",
        "    'bison',\n",
        "    'rocking chair, rocker',\n",
        "    'Christmas stocking',\n",
        "    'espresso',\n",
        "    'obelisk',\n",
        "    'tarantula',\n",
        "    'candle, taper, wax light',\n",
        "    'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
        "    'jellyfish',\n",
        "    'vestment',\n",
        "    'water tower',\n",
        "    'pop bottle, soda bottle',\n",
        "    'chimpanzee, chimp, Pan troglodytes',\n",
        "    'wok',\n",
        "    'pretzel',\n",
        "    'lampshade, lamp shade',\n",
        "    'turnstile',\n",
        "    'potpie',\n",
        "    'go-kart',\n",
        "    'suspension bridge',\n",
        "    'computer keyboard, keypad',\n",
        "    'snorkel',\n",
        "    'barbershop',\n",
        "    'banana',\n",
        "    'water jug',\n",
        "    'hourglass',\n",
        "    'ice lolly, lolly, lollipop, popsicle',\n",
        "    'cliff dwelling',\n",
        "    'hog, pig, grunter, squealer, Sus scrofa',\n",
        "    'sea cucumber, holothurian',\n",
        "    'king penguin, Aptenodytes patagonica',\n",
        "    'abacus',\n",
        "    'tabby, tabby cat',\n",
        "    'dining table, board',\n",
        "    'wooden spoon',\n",
        "    'acorn',\n",
        "    'bow tie, bow-tie, bowtie',\n",
        "    'pay-phone, pay-station',\n",
        "    'lion, king of beasts, Panthera leo',\n",
        "    'gazelle',\n",
        "    'chest',\n",
        "    'beaker',\n",
        "    'lawn mower, mower',\n",
        "    'confectionery, confectionary, candy store',\n",
        "    'lemon',\n",
        "    'pole',\n",
        "    'steel arch bridge',\n",
        "    'bullet train, bullet',\n",
        "    'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
        "    'mashed potato',\n",
        "    'alp',\n",
        "    'guinea pig, Cavia cobaya',\n",
        "    'trilobite',\n",
        "    'bathtub, bathing tub, bath, tub',\n",
        "    'tractor',\n",
        "    'sock',\n",
        "    'bucket, pail',\n",
        "    'jinrikisha, ricksha, rickshaw',\n",
        "    'kimono',\n",
        "    'binoculars, field glasses, opera glasses',\n",
        "    'pill bottle',\n",
        "    'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
        "    'snail',\n",
        "    'dugong, Dugong dugon',\n",
        "    'pizza, pizza pie',\n",
        "    'barrel, cask',\n",
        "    'brown bear, bruin, Ursus arctos',\n",
        "    'viaduct',\n",
        "    'gasmask, respirator, gas helmet',\n",
        "    \"plunger, plumber's helper\",\n",
        "    'moving van',\n",
        "    'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
        "    'broom',\n",
        "    'swimming trunks, bathing trunks',\n",
        "    'projectile, missile',\n",
        "    'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
        "    'brain coral',\n",
        "    'freight car',\n",
        "    'sports car, sport car',\n",
        "    'dam, dike, dyke',\n",
        "    'remote control, remote',\n",
        "    'sandal',\n",
        "    'school bus',\n",
        "    'fountain',\n",
        "    'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
        "    'mantis, mantid',\n",
        "    'scorpion',\n",
        "    'volleyball',\n",
        "    'nail',\n",
        "    'trolleybus, trolley coach, trackless trolley',\n",
        "    'Persian cat',\n",
        "    'lifeboat',\n",
        "    'teapot',\n",
        "    'crane',\n",
        "    'umbrella',\n",
        "    'lakeside, lakeshore',\n",
        "    'barn',\n",
        "    'organ, pipe organ',\n",
        "    'ice cream, icecream',\n",
        "    'Arabian camel, dromedary, Camelus dromedarius',\n",
        "    'oboe, hautboy, hautbois',\n",
        "    'reel',\n",
        "    'apron',\n",
        "    'beacon, lighthouse, beacon light, pharos',\n",
        "    'sombrero',\n",
        "    'flagpole, flagstaff',\n",
        "    'Egyptian cat',\n",
        "    'torch',\n",
        "    'bee',\n",
        "    'butcher shop, meat market',\n",
        "    'plate',\n",
        "    'fly',\n",
        "    'cardigan',\n",
        "    'ox',\n",
        "    \"potter's wheel\",\n",
        "    'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
        "    'seashore, coast, seacoast, sea-coast',\n",
        "    'rugby ball',\n",
        "    'iPod',\n",
        "    'African elephant, Loxodonta africana',\n",
        "    'teddy, teddy bear',\n",
        "    \"academic gown, academic robe, judge's robe\",\n",
        "    'birdhouse',\n",
        "    'bannister, banister, balustrade, balusters, handrail',\n",
        "    'magnetic compass',\n",
        "    'grasshopper, hopper',\n",
        "    'maypole',\n",
        "    'picket fence, paling',\n",
        "    'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
        "    'fur coat',\n",
        "    'neck brace',\n",
        "    'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
        "    'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
        "    'black widow, Latrodectus mactans'\n",
        "}\n",
        "\n",
        "filtered_data = [data for data in ds_train if idx_to_label[data['label']] in non_reptile_bird_dog_labels]\n",
        "print(len(filtered_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRutzaTr5WTr",
        "outputId": "53f0c07e-466e-4e19-cc16-e6e6c8352461"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "num_samples = 2000\n",
        "sampled_filtered_ds = ds_train.shuffle(seed=42).select(range(num_samples))\n",
        "label_counts = Counter([data['label'] for data in sampled_filtered_ds])\n",
        "counts = list(label_counts.values())\n",
        "\n",
        "print(f\"Total labels: {sum(counts)}\")\n",
        "print(f\"Unique labels: {len(label_counts)}\")\n",
        "print(f\"Max samples per label: {max(counts)}\")\n",
        "print(f\"Min samples per label: {min(counts)}\")\n",
        "print(f\"Average samples per label: {sum(counts) / len(counts):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMJ-FOig9FB6",
        "outputId": "1f7df9a1-0db2-45f3-b210-f1e180d68208"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total labels: 2000\n",
            "Unique labels: 200\n",
            "Max samples per label: 18\n",
            "Min samples per label: 3\n",
            "Average samples per label: 10.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# move these novel samples to train_image folder\n",
        "oe_image_dir = base_dir + \"data/train_images/\"\n",
        "\n",
        "oe_rows = []\n",
        "for i, item in tqdm(enumerate(sampled_filtered_ds), total=len(sampled_filtered_ds)):\n",
        "    image = item[\"image\"]\n",
        "    filename = f\"novel_{i}.jpg\"\n",
        "    image_path = os.path.join(oe_image_dir, filename)\n",
        "    image.save(image_path)\n",
        "\n",
        "    oe_rows.append({\n",
        "        \"image\": filename,\n",
        "        \"superclass_index\": 3,\n",
        "        \"subclass_index\": 87,\n",
        "        \"superclass_name\": \"novel\",\n",
        "        \"subclass_name\": \"novel\"\n",
        "    })\n",
        "\n",
        "novel_df = pd.DataFrame(oe_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-ktxnezH-zg",
        "outputId": "ec370443-6f72-4930-8c3d-04162096e144"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [01:12<00:00, 27.47it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utlity Helper Methods"
      ],
      "metadata": {
        "id": "Wuwyvdc067x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "# Used AI to automate printing the key stats here, but logic for accumulating and calcualting the stats was by us\n",
        "def analyze_prediction_distributions(prediction_distributions):\n",
        "    # === Confidence Stats ===\n",
        "    for key in [\"super_confidence\", \"sub_confidence\"]:\n",
        "        values = np.array(prediction_distributions[key])\n",
        "        print(f\"\\n Stats for {key.replace('_', ' ').title()}:\")\n",
        "        print(f\"  Count:     {len(values)}\")\n",
        "        print(f\"  Mean:      {values.mean():.4f}\")\n",
        "        print(f\"  Std Dev:   {values.std():.4f}\")\n",
        "        print(f\"  Min:       {values.min():.4f}\")\n",
        "        print(f\"  Max:       {values.max():.4f}\")\n",
        "        print(f\"  25th pct:  {np.percentile(values, 25):.4f}\")\n",
        "        print(f\"  Median:    {np.median(values):.4f}\")\n",
        "        print(f\"  75th pct:  {np.percentile(values, 75):.4f}\")\n",
        "        below_50 = (values < 0.5).sum()\n",
        "        print(f\"  Below 0.5: {below_50} samples ({below_50 / len(values) * 100:.2f}%)\")\n",
        "\n",
        "    # === Prediction Counts ===\n",
        "    print(\"\\n Raw Superclass Prediction Distribution:\")\n",
        "    super_counts = Counter(prediction_distributions[\"raw_superclass_pred\"])\n",
        "    for label, count in sorted(super_counts.items()):\n",
        "        print(f\"  Superclass {label}: {count} samples\")\n",
        "\n",
        "    print(\"\\n Raw Subclass Prediction Distribution (Top 15):\")\n",
        "    subclass_counts = Counter(prediction_distributions[\"raw_subclass_pred\"])\n",
        "    for label, count in subclass_counts.most_common(15):\n",
        "        print(f\"  Subclass {label}: {count} samples\")\n",
        "\n",
        "\n",
        "    print(\"\\n Raw Subclass Prediction Distribution (Bottom 15):\")\n",
        "    subclass_counts = Counter(prediction_distributions[\"raw_subclass_pred\"])\n",
        "    for label, count in sorted(subclass_counts.items(), key=lambda x: x[1])[:15]:\n",
        "        print(f\"  Subclass {label}: {count} samples\")"
      ],
      "metadata": {
        "id": "7sQNvzlNCBwK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# visualize the predictions\n",
        "def print_confidence_stats(confidences, label):\n",
        "    confidences = np.array(confidences)\n",
        "    print(f\"\\n Stats for {label} confidence:\")\n",
        "    print(f\"  Mean:      {confidences.mean():.4f}\")\n",
        "    print(f\"  Std Dev:   {confidences.std():.4f}\")\n",
        "    print(f\"  Min:       {confidences.min():.4f}\")\n",
        "    print(f\"  Max:       {confidences.max():.4f}\")\n",
        "    print(f\"  25th pct:  {np.percentile(confidences, 25):.4f}\")\n",
        "    print(f\"  50th pct:  {np.percentile(confidences, 50):.4f} (median)\")\n",
        "    print(f\"  75th pct:  {np.percentile(confidences, 75):.4f}\")\n",
        "    print(f\"  Below 0.5: {(confidences < 0.5).sum()} samples ({(confidences < 0.5).mean()*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "jb-QaBNLH3i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(transform, use_novel_data=True):\n",
        "    \"\"\"\n",
        "    Given a transform and whether to use novel data in training,\n",
        "    returns training and validation datasets\n",
        "    \"\"\"\n",
        "    training_data = train_df if not use_novel_data else pd.concat([train_df, novel_df], ignore_index=True)\n",
        "\n",
        "    full_dataset = MultiClassImageDataset(training_data, img_dir=image_dir, transform=transform)\n",
        "\n",
        "    train_size = int(0.9 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_dataset, [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "QGsqzBtTJrQ0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import RandAugment\n",
        "def get_transform(data_augmentation=False, model=\"Resnet50\"):\n",
        "    if model == \"Resnet50\":\n",
        "        base_transforms = [\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                 std=(0.229, 0.224, 0.225))\n",
        "        ]\n",
        "\n",
        "        if data_augmentation:\n",
        "            aug_transforms = [\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(degrees=15),\n",
        "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "                transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0))\n",
        "                # RandAugment(num_ops=2, magnitude=9),\n",
        "            ]\n",
        "            return transforms.Compose(aug_transforms + base_transforms)\n",
        "        else:\n",
        "            return transforms.Compose(base_transforms)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"No transform configured for model '{model}'\")\n"
      ],
      "metadata": {
        "id": "jkfb_zL39KN3"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train ResNet50"
      ],
      "metadata": {
        "id": "smE37qNUxOIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineClassifier(nn.Module):\n",
        "    def __init__(self, in_features, num_classes, scale=10.0):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(num_classes, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = nn.functional.normalize(x, dim=1)\n",
        "        w_norm = nn.functional.normalize(self.weight, dim=1)\n",
        "        return self.scale * torch.matmul(x_norm, w_norm.T)"
      ],
      "metadata": {
        "id": "nC3CdIzW8IqP"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class ResNet50MultiHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_superclasses=4,\n",
        "        num_subclasses=88,\n",
        "        use_dropout=False,\n",
        "        dropout_p=0.5,\n",
        "        use_nonlinear_head=False,\n",
        "        use_cosine_classifier=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        ResNet-50 model with dual classification heads for hierarchical prediction.\n",
        "\n",
        "        Args:\n",
        "            num_superclasses (int): Number of classes for the superclass head.\n",
        "            num_subclasses (int): Number of classes for the subclass head.\n",
        "            use_dropout (bool): If True, applies dropout between layers in MLP heads.\n",
        "            dropout_p (float): Dropout probability used if use_dropout is True.\n",
        "            use_nonlinear_head (bool): If True, each head is a 2-layer MLP; otherwise, a single linear layer.\n",
        "            use_cosine_classifier (bool): If True, replaces both heads with cosine similarity classifiers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Freeze most layers except last block\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if \"layer4\" in name or \"avgpool\" in name:\n",
        "                param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.embedding_dim = 2048\n",
        "        self.use_dropout = use_dropout\n",
        "        self.use_nonlinear_head = use_nonlinear_head\n",
        "        self.use_cosine_classifier = use_cosine_classifier\n",
        "\n",
        "        # === Superclass Head ===\n",
        "        if self.use_cosine_classifier:\n",
        "            self.superclass_head = CosineClassifier(self.embedding_dim, num_superclasses)\n",
        "        elif self.use_nonlinear_head:\n",
        "            self.superclass_head = nn.Sequential(\n",
        "                nn.Linear(self.embedding_dim, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=dropout_p) if use_dropout else nn.Identity(),\n",
        "                nn.Linear(512, num_superclasses)\n",
        "            )\n",
        "        else:\n",
        "            self.superclass_head = nn.Linear(self.embedding_dim, num_superclasses)\n",
        "\n",
        "        # === Subclass Head ===\n",
        "        if self.use_cosine_classifier:\n",
        "            self.subclass_head = CosineClassifier(self.embedding_dim, num_subclasses)\n",
        "        elif self.use_nonlinear_head:\n",
        "            self.subclass_head = nn.Sequential(\n",
        "                nn.Linear(self.embedding_dim, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=dropout_p) if use_dropout else nn.Identity(),\n",
        "                nn.Linear(512, num_subclasses)\n",
        "            )\n",
        "        else:\n",
        "            self.subclass_head = nn.Linear(self.embedding_dim, num_subclasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        super_logits = self.superclass_head(features)\n",
        "        sub_logits = self.subclass_head(features)\n",
        "        return super_logits, sub_logits"
      ],
      "metadata": {
        "id": "vKj8We96xP14"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used AI to give boilder plate training code, then we customized it for our use case\n",
        "# === Prediction tracking for analysis ===\n",
        "resnet_training_prediction_distributions = {\n",
        "  \"raw_superclass_pred\": [],\n",
        "  \"raw_subclass_pred\": [],\n",
        "    \"super_confidence\": [],\n",
        "    \"sub_confidence\": []\n",
        "  }\n",
        "\n",
        "\n",
        "# Code block to train Resnet\n",
        "# === Code block to train ResNet-50 ===\n",
        "def train_resnet50(use_novel_data=True, use_nonlinear_head=False, use_augmentation=False, use_cosine_classifier=False, use_dropout=False, dropout_p=0.5, num_epochs=20):\n",
        "    \"\"\"\n",
        "    Trains a ResNet-50 model with dual heads for hierarchical classification (superclass + subclass).\n",
        "\n",
        "    Args:\n",
        "        use_novel_data (bool): If True, includes novel-class images in training (for outlier exposure).\n",
        "        use_nonlinear_head (bool): If True, uses MLP classifier heads instead of linear.\n",
        "        use_augmentation (bool): If True, applies data augmentation during training.\n",
        "        use_cosine_classifier (bool): If True, uses cosine similarity classifier heads.\n",
        "        use_dropout (bool): If True, applies dropout in the classifier heads.\n",
        "        dropout_p (float): Dropout probability if dropout is enabled.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "\n",
        "    Returns:\n",
        "        model (ResNet50MultiHead): The trained ResNet-50 model with two classification heads.\n",
        "    \"\"\"\n",
        "    # === Image transforms ===\n",
        "    image_transforms = get_transform(data_augmentation=use_augmentation, model=\"Resnet50\")\n",
        "\n",
        "    # === Load dataset ===\n",
        "    train_dataset, val_dataset = get_dataset(transform=image_transforms, use_novel_data=use_novel_data)\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    # === Model, Loss, Optimizer Setup ===\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    model = ResNet50MultiHead(use_nonlinear_head=use_nonlinear_head, use_cosine_classifier=use_cosine_classifier, use_dropout=use_dropout, dropout_p=dropout_p).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # === Training loop ===\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, super_labels, sub_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            images = images.to(device)\n",
        "            super_labels = super_labels.to(device)\n",
        "            sub_labels = sub_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            super_logits, sub_logits = model(images)\n",
        "\n",
        "            super_loss = criterion(super_logits, super_labels)\n",
        "            sub_loss = criterion(sub_logits, sub_labels)\n",
        "            loss = super_loss + sub_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # === Validation ===\n",
        "        model.eval()\n",
        "        super_correct = sub_correct = total = 0\n",
        "        super_loss_total, sub_loss_total = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, super_labels, sub_labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                super_logits, sub_logits = model(images)\n",
        "\n",
        "                # === Compute and accumulate validation loss\n",
        "                super_loss = criterion(super_logits, super_labels)\n",
        "                sub_loss = criterion(sub_logits, sub_labels)\n",
        "                super_loss_total += super_loss.item()\n",
        "                sub_loss_total += sub_loss.item()\n",
        "\n",
        "                super_preds = torch.argmax(super_logits, dim=1)\n",
        "                sub_preds = torch.argmax(sub_logits, dim=1)\n",
        "\n",
        "                super_correct += (super_preds == super_labels).sum().item()\n",
        "                sub_correct += (sub_preds == sub_labels).sum().item()\n",
        "                total += images.size(0)\n",
        "\n",
        "                # Confidence tracking\n",
        "                super_probs = torch.softmax(super_logits, dim=1)\n",
        "                sub_probs = torch.softmax(sub_logits, dim=1)\n",
        "                super_conf, _ = torch.max(super_probs, dim=1)\n",
        "                sub_conf, _ = torch.max(sub_probs, dim=1)\n",
        "\n",
        "                resnet_training_prediction_distributions[\"raw_superclass_pred\"].extend(super_preds.cpu().tolist())\n",
        "                resnet_training_prediction_distributions[\"raw_subclass_pred\"].extend(sub_preds.cpu().tolist())\n",
        "                resnet_training_prediction_distributions[\"super_confidence\"].extend(super_conf.cpu().tolist())\n",
        "                resnet_training_prediction_distributions[\"sub_confidence\"].extend(sub_conf.cpu().tolist())\n",
        "\n",
        "        avg_super_loss = super_loss_total / len(val_loader)\n",
        "        avg_sub_loss = sub_loss_total / len(val_loader)\n",
        "\n",
        "        print(f\"Validation Accuracy | Superclass: {super_correct / total:.4f} | Subclass: {sub_correct / total:.4f}\")\n",
        "        print(f\"Validation Loss     | Superclass: {avg_super_loss:.4f} | Subclass: {avg_sub_loss:.4f}\")\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "90deByTI-hAJ"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_resnet50(# can modify params here\n",
        "    use_novel_data=True,\n",
        "    use_augmentation=False,\n",
        "    use_nonlinear_head=False,\n",
        "    use_dropout=False,\n",
        "    dropout_p=0.5,\n",
        "    use_cosine_classifier=False,\n",
        "    num_epochs=20\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JLm0sux_NrC",
        "outputId": "3ee01371-b56a-471a-aa86-4c0c50069d0d"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1: 100%|██████████| 117/117 [00:31<00:00,  3.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 1.9419\n",
            "Validation Accuracy | Superclass: 0.9976 | Subclass: 0.8745\n",
            "Validation Loss     | Superclass: 0.0182 | Subclass: 0.5704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 117/117 [00:30<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 | Train Loss: 0.3536\n",
            "Validation Accuracy | Superclass: 0.9976 | Subclass: 0.9457\n",
            "Validation Loss     | Superclass: 0.0133 | Subclass: 0.2083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 117/117 [00:30<00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 | Train Loss: 0.1134\n",
            "Validation Accuracy | Superclass: 0.9976 | Subclass: 0.9517\n",
            "Validation Loss     | Superclass: 0.0080 | Subclass: 0.1610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 117/117 [00:31<00:00,  3.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 | Train Loss: 0.0499\n",
            "Validation Accuracy | Superclass: 0.9988 | Subclass: 0.9590\n",
            "Validation Loss     | Superclass: 0.0073 | Subclass: 0.1319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 117/117 [00:30<00:00,  3.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 | Train Loss: 0.0226\n",
            "Validation Accuracy | Superclass: 0.9988 | Subclass: 0.9698\n",
            "Validation Loss     | Superclass: 0.0042 | Subclass: 0.1101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 117/117 [00:30<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 | Train Loss: 0.0224\n",
            "Validation Accuracy | Superclass: 1.0000 | Subclass: 0.9614\n",
            "Validation Loss     | Superclass: 0.0019 | Subclass: 0.1427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 117/117 [00:30<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 | Train Loss: 0.0154\n",
            "Validation Accuracy | Superclass: 1.0000 | Subclass: 0.9650\n",
            "Validation Loss     | Superclass: 0.0026 | Subclass: 0.1136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 117/117 [00:30<00:00,  3.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 | Train Loss: 0.0076\n",
            "Validation Accuracy | Superclass: 0.9988 | Subclass: 0.9698\n",
            "Validation Loss     | Superclass: 0.0033 | Subclass: 0.0984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 117/117 [00:30<00:00,  3.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 | Train Loss: 0.0053\n",
            "Validation Accuracy | Superclass: 1.0000 | Subclass: 0.9698\n",
            "Validation Loss     | Superclass: 0.0020 | Subclass: 0.1001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 117/117 [00:30<00:00,  3.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 | Train Loss: 0.0123\n",
            "Validation Accuracy | Superclass: 0.9964 | Subclass: 0.9590\n",
            "Validation Loss     | Superclass: 0.0093 | Subclass: 0.1271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 117/117 [00:30<00:00,  3.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 | Train Loss: 0.0161\n",
            "Validation Accuracy | Superclass: 0.9988 | Subclass: 0.9698\n",
            "Validation Loss     | Superclass: 0.0044 | Subclass: 0.1050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 117/117 [00:30<00:00,  3.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 | Train Loss: 0.0047\n",
            "Validation Accuracy | Superclass: 0.9988 | Subclass: 0.9735\n",
            "Validation Loss     | Superclass: 0.0029 | Subclass: 0.0977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 117/117 [00:30<00:00,  3.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 | Train Loss: 0.0039\n",
            "Validation Accuracy | Superclass: 0.9988 | Subclass: 0.9698\n",
            "Validation Loss     | Superclass: 0.0037 | Subclass: 0.0978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 117/117 [00:30<00:00,  3.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 | Train Loss: 0.0023\n",
            "Validation Accuracy | Superclass: 1.0000 | Subclass: 0.9710\n",
            "Validation Loss     | Superclass: 0.0014 | Subclass: 0.0964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 117/117 [00:30<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 | Train Loss: 0.0031\n",
            "Validation Accuracy | Superclass: 0.9988 | Subclass: 0.9698\n",
            "Validation Loss     | Superclass: 0.0063 | Subclass: 0.1012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 117/117 [00:30<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 | Train Loss: 0.0025\n",
            "Validation Accuracy | Superclass: 1.0000 | Subclass: 0.9747\n",
            "Validation Loss     | Superclass: 0.0015 | Subclass: 0.0988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 117/117 [00:30<00:00,  3.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 | Train Loss: 0.0027\n",
            "Validation Accuracy | Superclass: 0.9988 | Subclass: 0.9735\n",
            "Validation Loss     | Superclass: 0.0017 | Subclass: 0.1037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 117/117 [00:30<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 | Train Loss: 0.0016\n",
            "Validation Accuracy | Superclass: 1.0000 | Subclass: 0.9710\n",
            "Validation Loss     | Superclass: 0.0004 | Subclass: 0.1015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 117/117 [00:30<00:00,  3.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 | Train Loss: 0.0012\n",
            "Validation Accuracy | Superclass: 1.0000 | Subclass: 0.9723\n",
            "Validation Loss     | Superclass: 0.0009 | Subclass: 0.0962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████| 117/117 [00:30<00:00,  3.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 | Train Loss: 0.0011\n",
            "Validation Accuracy | Superclass: 1.0000 | Subclass: 0.9710\n",
            "Validation Loss     | Superclass: 0.0009 | Subclass: 0.0943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the prediction distributions\n",
        "analyze_prediction_distributions(resnet_training_prediction_distributions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zENB-wMfH9KG",
        "outputId": "2e50485a-e084-42c3-918a-111edb5e4bc9"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Stats for Super Confidence:\n",
            "  Count:     74900\n",
            "  Mean:      0.9796\n",
            "  Std Dev:   0.0554\n",
            "  Min:       0.3623\n",
            "  Max:       1.0000\n",
            "  25th pct:  0.9839\n",
            "  Median:    0.9982\n",
            "  75th pct:  0.9998\n",
            "  Below 0.5: 112 samples (0.15%)\n",
            "\n",
            " Stats for Sub Confidence:\n",
            "  Count:     74900\n",
            "  Mean:      0.8324\n",
            "  Std Dev:   0.2587\n",
            "  Min:       0.0198\n",
            "  Max:       1.0000\n",
            "  25th pct:  0.7786\n",
            "  Median:    0.9663\n",
            "  75th pct:  0.9975\n",
            "  Below 0.5: 10388 samples (13.87%)\n",
            "\n",
            " Raw Superclass Prediction Distribution:\n",
            "  Superclass 0: 18904 samples\n",
            "  Superclass 1: 20751 samples\n",
            "  Superclass 2: 24457 samples\n",
            "  Superclass 3: 10788 samples\n",
            "\n",
            " Raw Subclass Prediction Distribution (Top 15):\n",
            "  Subclass 87: 10809 samples\n",
            "  Subclass 76: 1495 samples\n",
            "  Subclass 69: 1427 samples\n",
            "  Subclass 63: 1402 samples\n",
            "  Subclass 29: 1369 samples\n",
            "  Subclass 6: 1338 samples\n",
            "  Subclass 30: 1308 samples\n",
            "  Subclass 66: 1270 samples\n",
            "  Subclass 21: 1210 samples\n",
            "  Subclass 49: 1207 samples\n",
            "  Subclass 27: 1195 samples\n",
            "  Subclass 37: 1178 samples\n",
            "  Subclass 47: 1147 samples\n",
            "  Subclass 44: 1129 samples\n",
            "  Subclass 75: 1107 samples\n",
            "\n",
            " Raw Subclass Prediction Distribution (Bottom 15):\n",
            "  Subclass 39: 222 samples\n",
            "  Subclass 73: 280 samples\n",
            "  Subclass 25: 280 samples\n",
            "  Subclass 45: 281 samples\n",
            "  Subclass 0: 326 samples\n",
            "  Subclass 74: 326 samples\n",
            "  Subclass 9: 349 samples\n",
            "  Subclass 14: 354 samples\n",
            "  Subclass 8: 373 samples\n",
            "  Subclass 11: 374 samples\n",
            "  Subclass 78: 378 samples\n",
            "  Subclass 48: 392 samples\n",
            "  Subclass 19: 397 samples\n",
            "  Subclass 23: 397 samples\n",
            "  Subclass 13: 409 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating CSV for Submitting to Leaderboard"
      ],
      "metadata": {
        "id": "mnoVoEGO9yG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using these to check the distributions of predictions\n",
        "test_prediction_distributions = {\n",
        "    \"raw_superclass_pred\": [],\n",
        "    \"raw_subclass_pred\": [],\n",
        "    \"super_confidence\": [],\n",
        "    \"sub_confidence\": [],\n",
        "    \"super_logit\": [],\n",
        "    \"sub_logit\": []\n",
        "}\n",
        "\n",
        "# These are better if trained on novel data\n",
        "superclass_cutoff = 0.95 # if bleow this predict novel\n",
        "subclass_cutoff = 0.6 # if below this predict novel\n",
        "\n",
        "super_logit_percentile_cutoff = 35\n",
        "sub_logit_percentile_cutoff = 50\n",
        "\n",
        "\n",
        "# These are better if trained not on novel\n",
        "# superclass_cutoff = 0.98 # if bleow this predict novel (better for when not trained with novel data)\n",
        "# subclass_cutoff = 0.75 # if below this predict novel\n",
        "\n",
        "# === Submission Function ===\n",
        "def generate_submission_csv(model, test_dir, output_path, transform, device, include_cutoffs=True):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    test_dataset = MultiClassImageTestDataset(img_dir=test_dir, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    predictions = {\n",
        "        \"image\": [],\n",
        "        \"superclass_index\": [],\n",
        "        \"subclass_index\": []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, img_names in tqdm(test_loader):\n",
        "          images = images.to(device)\n",
        "          super_logits, sub_logits = model(images)\n",
        "\n",
        "          super_probs = torch.softmax(super_logits, dim=1)\n",
        "          sub_probs = torch.softmax(sub_logits, dim=1)\n",
        "\n",
        "          super_logit_val, _ = torch.max(super_logits, dim=1)\n",
        "          sub_logit_val, _ = torch.max(sub_logits, dim=1)\n",
        "\n",
        "          super_conf, super_preds = torch.max(super_probs, dim=1)\n",
        "          sub_conf, sub_preds = torch.max(sub_probs, dim=1)\n",
        "\n",
        "          # Record raw values for analysis\n",
        "          test_prediction_distributions[\"raw_superclass_pred\"].append(super_preds.item())\n",
        "          test_prediction_distributions[\"raw_subclass_pred\"].append(sub_preds.item())\n",
        "          test_prediction_distributions[\"super_confidence\"].append(super_conf.item())\n",
        "          test_prediction_distributions[\"sub_confidence\"].append(sub_conf.item())\n",
        "          test_prediction_distributions[\"super_logit\"].append(super_logit_val.item())\n",
        "          test_prediction_distributions[\"sub_logit\"].append(sub_logit_val.item())\n",
        "\n",
        "          # === Logit Thresholds ===\n",
        "\n",
        "          # Add novel class detection threshold\n",
        "          if include_cutoffs:\n",
        "            if super_conf.item() < superclass_cutoff:\n",
        "                super_preds[0] = 3\n",
        "            if super_preds[0] == 3 or sub_conf.item() < subclass_cutoff:\n",
        "                sub_preds[0] = 87\n",
        "\n",
        "          predictions[\"image\"].append(img_names[0])\n",
        "          predictions[\"superclass_index\"].append(super_preds.item())\n",
        "          predictions[\"subclass_index\"].append(sub_preds.item())\n",
        "\n",
        "    if include_cutoffs:\n",
        "      # === Compute 25th percentile logit thresholds ===\n",
        "      super_logit_array = np.array(test_prediction_distributions[\"super_logit\"])\n",
        "      sub_logit_array = np.array(test_prediction_distributions[\"sub_logit\"])\n",
        "\n",
        "      super_logit_thresh = np.percentile(super_logit_array, super_logit_percentile_cutoff)\n",
        "      sub_logit_thresh = np.percentile(sub_logit_array, sub_logit_percentile_cutoff)\n",
        "\n",
        "      print(f\"Logit thresholds — Super: {super_logit_thresh:.4f}, Sub: {sub_logit_thresh:.4f}\")\n",
        "\n",
        "      # === Generate second CSV using raw logit thresholds ===\n",
        "      logit_predictions = {\n",
        "          \"image\": [],\n",
        "          \"superclass_index\": [],\n",
        "          \"subclass_index\": []\n",
        "      }\n",
        "\n",
        "      for i in range(len(super_logit_array)):\n",
        "          super_pred = test_prediction_distributions[\"raw_superclass_pred\"][i]\n",
        "          sub_pred = test_prediction_distributions[\"raw_subclass_pred\"][i]\n",
        "          super_logit = super_logit_array[i]\n",
        "          sub_logit = sub_logit_array[i]\n",
        "\n",
        "          if super_logit < super_logit_thresh:\n",
        "              super_pred = 3\n",
        "          if super_pred == 3 or sub_logit < sub_logit_thresh:\n",
        "              sub_pred = 87\n",
        "\n",
        "          logit_predictions[\"image\"].append(predictions[\"image\"][i])\n",
        "          logit_predictions[\"superclass_index\"].append(super_pred)\n",
        "          logit_predictions[\"subclass_index\"].append(sub_pred)\n",
        "\n",
        "      # === Save logit-based submission ===\n",
        "      logit_output_path = output_path.replace(\".csv\", \"_logit.csv\")\n",
        "      pd.DataFrame(logit_predictions).to_csv(logit_output_path, index=False)\n",
        "      print(f\"Saved logit-based submission: {logit_output_path}\")\n",
        "\n",
        "    # === Save softmax-based submission with suffix ===\n",
        "    softmax_output_path = output_path.replace(\".csv\", \"_softmax.csv\")\n",
        "    pd.DataFrame(predictions).to_csv(softmax_output_path, index=False)\n",
        "    print(f\"Saved softmax-based submission to: {softmax_output_path}\")"
      ],
      "metadata": {
        "id": "lcpljDS593kj"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "generate_submission_csv(\n",
        "    model=model,\n",
        "    test_dir=\"/content/drive/MyDrive/COMS_4995/final_project/data/test_images\",\n",
        "    output_path=\"/content/drive/MyDrive/COMS_4995/final_project/results/test_predictions.csv\",\n",
        "    transform=get_transform(data_augmentation=False, model=\"Resnet50\"), # this code is for resnet\n",
        "    # transform=preprocess, # transorm needed for clip\n",
        "    # transform=efficient_net_transforms, # transform needed for efficientnet\n",
        "    device=device,\n",
        "    include_cutoffs=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuoSD4RX-uo6",
        "outputId": "0d0e3cca-6739-4e79-bd1c-3c3792988017"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11180/11180 [03:45<00:00, 49.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logit thresholds — Super: 6.3420, Sub: 4.7054\n",
            "Saved logit-based submission: /content/drive/MyDrive/COMS_4995/final_project/results/test_predictions_logit.csv\n",
            "Saved softmax-based submission to: /content/drive/MyDrive/COMS_4995/final_project/results/test_predictions_softmax.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_prediction_distributions(test_prediction_distributions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMoOx5_acl0v",
        "outputId": "f82ea0dd-893b-4757-9ca2-2763bc7b59e5"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Stats for Super Confidence:\n",
            "  Count:     11180\n",
            "  Mean:      0.9509\n",
            "  Std Dev:   0.1189\n",
            "  Min:       0.3322\n",
            "  Max:       1.0000\n",
            "  25th pct:  0.9918\n",
            "  Median:    0.9999\n",
            "  75th pct:  1.0000\n",
            "  Below 0.5: 158 samples (1.41%)\n",
            "\n",
            " Stats for Sub Confidence:\n",
            "  Count:     11180\n",
            "  Mean:      0.7065\n",
            "  Std Dev:   0.2743\n",
            "  Min:       0.0599\n",
            "  Max:       1.0000\n",
            "  25th pct:  0.4660\n",
            "  Median:    0.7734\n",
            "  75th pct:  0.9793\n",
            "  Below 0.5: 3148 samples (28.16%)\n",
            "\n",
            " Raw Superclass Prediction Distribution:\n",
            "  Superclass 0: 4135 samples\n",
            "  Superclass 1: 2992 samples\n",
            "  Superclass 2: 3440 samples\n",
            "  Superclass 3: 613 samples\n",
            "\n",
            " Raw Subclass Prediction Distribution (Top 15):\n",
            "  Subclass 87: 1703 samples\n",
            "  Subclass 16: 604 samples\n",
            "  Subclass 78: 421 samples\n",
            "  Subclass 20: 274 samples\n",
            "  Subclass 45: 263 samples\n",
            "  Subclass 64: 262 samples\n",
            "  Subclass 1: 255 samples\n",
            "  Subclass 59: 247 samples\n",
            "  Subclass 39: 241 samples\n",
            "  Subclass 49: 230 samples\n",
            "  Subclass 11: 213 samples\n",
            "  Subclass 46: 207 samples\n",
            "  Subclass 54: 207 samples\n",
            "  Subclass 86: 206 samples\n",
            "  Subclass 57: 199 samples\n",
            "\n",
            " Raw Subclass Prediction Distribution (Bottom 15):\n",
            "  Subclass 66: 1 samples\n",
            "  Subclass 76: 2 samples\n",
            "  Subclass 36: 2 samples\n",
            "  Subclass 6: 4 samples\n",
            "  Subclass 44: 6 samples\n",
            "  Subclass 81: 9 samples\n",
            "  Subclass 29: 10 samples\n",
            "  Subclass 69: 11 samples\n",
            "  Subclass 37: 12 samples\n",
            "  Subclass 31: 17 samples\n",
            "  Subclass 27: 29 samples\n",
            "  Subclass 75: 29 samples\n",
            "  Subclass 22: 30 samples\n",
            "  Subclass 61: 31 samples\n",
            "  Subclass 21: 32 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BgJqdt_6-t_V"
      }
    }
  ]
}