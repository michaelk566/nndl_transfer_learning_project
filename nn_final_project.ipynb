{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Persist data on google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D0Rt37qm3WP",
        "outputId": "162f73a9-ed5e-43e7-bcae-b897dfbdf5ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive/COMS_4995/final_project/\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "NwjLsd9zpLJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a862565-8ff9-4cdc-cd04-786f90dd86b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-c7q2uf4v\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-c7q2uf4v\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Required Data"
      ],
      "metadata": {
        "id": "qE9OjQbwIQ2q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9rV7SF0lDp4",
        "outputId": "6f82be97-9bc8-4fba-fee8-4325955ba434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_images/ folder already exists. Skipping unzip.\n",
            "test_images/ folder already exists. Skipping unzip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/COMS_4995/final_project/'\n",
        "image_dir = base_dir + 'data/train_images/'\n",
        "zip_path = base_dir + \"data/train_images.zip\"\n",
        "\n",
        "train_zip_path = base_dir + \"data/train_images.zip\"\n",
        "train_extract_dir = base_dir + \"data/\"\n",
        "test_zip_path = base_dir + \"data/test_images.zip\"\n",
        "test_extract_dir = base_dir + \"data/\"\n",
        "\n",
        "# === Unzip train_images if not already extracted ===\n",
        "if not os.path.exists(train_extract_dir):\n",
        "    print(\"Unzipping train_images.zip...\")\n",
        "    with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(train_extract_dir)\n",
        "    print(\"Unzipped to:\", train_extract_dir)\n",
        "else:\n",
        "    print(\"train_images/ folder already exists. Skipping unzip.\")\n",
        "\n",
        "# === Unzip test_images if not already extracted ===\n",
        "if not os.path.exists(test_extract_dir):\n",
        "    print(\"Unzipping test_images.zip...\")\n",
        "    with zipfile.ZipFile(test_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(test_extract_dir)\n",
        "    print(\"Unzipped to:\", test_extract_dir)\n",
        "else:\n",
        "    print(\"test_images/ folder already exists. Skipping unzip.\")\n",
        "\n",
        "# Load CSVs\n",
        "train_df = pd.read_csv(base_dir + \"data/train_data.csv\")\n",
        "superclass_map = pd.read_csv(base_dir + \"data/superclass_mapping.csv\")\n",
        "subclass_map = pd.read_csv(base_dir + \"data/subclass_mapping.csv\")\n",
        "\n",
        "# Merge labels for readability\n",
        "train_df = train_df.merge(superclass_map, left_on=\"superclass_index\", right_index=True, suffixes=('', '_super'))\n",
        "train_df = train_df.merge(subclass_map, left_on=\"subclass_index\", right_index=True, suffixes=('', '_sub'))\n",
        "\n",
        "# Rename for clarity\n",
        "train_df.rename(columns={\"class\": \"superclass_name\", \"class_sub\": \"subclass_name\"}, inplace=True)\n",
        "train_df = train_df.drop(columns=[\"index\", \"index_sub\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore/Visualize Training Data"
      ],
      "metadata": {
        "id": "iI6mcVe1mpRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "total_images = len(train_df)\n",
        "print(\"Total training images:\", total_images)\n",
        "\n",
        "num_superclasses = train_df['superclass_name'].nunique()\n",
        "num_subclasses = train_df['subclass_name'].nunique()\n",
        "print(\"Unique superclasses:\", num_superclasses)\n",
        "print(\"Unique subclasses:\", num_subclasses)\n",
        "\n",
        "# === Superclass Counts with Percentages ===\n",
        "superclass_counts = train_df['superclass_name'].value_counts().reset_index()\n",
        "superclass_counts.columns = ['Superclass', 'Image Count']\n",
        "superclass_counts['Percent of Total'] = (superclass_counts['Image Count'] / total_images * 100).round(2)\n",
        "\n",
        "print(\"\\nImages per Superclass:\")\n",
        "print(superclass_counts)\n",
        "\n",
        "# === Group by (superclass, subclass) ===\n",
        "grouped = train_df.groupby(['superclass_name', 'subclass_name']).size().reset_index(name='Image Count')\n",
        "\n",
        "# Total per superclass (for per-superclass percentages)\n",
        "super_totals = train_df.groupby('superclass_name').size().to_dict()\n",
        "total_images = len(train_df)\n",
        "\n",
        "# Compute percentages\n",
        "grouped['% of Superclass'] = grouped.apply(\n",
        "    lambda row: (row['Image Count'] / super_totals[row['superclass_name']] * 100), axis=1\n",
        ").round(2)\n",
        "\n",
        "grouped['% of Total'] = (grouped['Image Count'] / total_images * 100).round(2)\n",
        "\n",
        "# Sort and display\n",
        "grouped = grouped.sort_values(by=['superclass_name', 'Image Count'], ascending=[True, False])\n",
        "\n",
        "# === Global (across all data) stats ===\n",
        "global_percents = grouped['% of Total'].values\n",
        "print(\"\\n Stats on % of Total Dataset per (superclass, subclass):\")\n",
        "print(f\"  Min:    {global_percents.min():.2f}%\")\n",
        "print(f\"  Max:    {global_percents.max():.2f}%\")\n",
        "print(f\"  Mean:   {global_percents.mean():.2f}%\")\n",
        "print(f\"  Median: {np.median(global_percents):.2f}%\")\n",
        "print(f\"  Std:    {global_percents.std():.2f}%\")\n",
        "\n",
        "# === Per-superclass relative stats ===\n",
        "relative_percents = grouped['% of Superclass'].values\n",
        "print(\"\\n Stats on % of Superclass per subclass:\")\n",
        "print(f\"  Min:    {relative_percents.min():.2f}%\")\n",
        "print(f\"  Max:    {relative_percents.max():.2f}%\")\n",
        "print(f\"  Mean:   {relative_percents.mean():.2f}%\")\n",
        "print(f\"  Median: {np.median(relative_percents):.2f}%\")\n",
        "print(f\"  Std:    {relative_percents.std():.2f}%\")\n",
        "\n",
        "\n",
        "superclasses = grouped['superclass_name'].unique()\n",
        "\n",
        "for sc in superclasses:\n",
        "    print(f\"\\n Stats for subclasses within superclass: {sc}\")\n",
        "\n",
        "    subset = grouped[grouped['superclass_name'] == sc]['% of Superclass'].values\n",
        "    print(f\"  Number of subclasses: {len(subset)}\")\n",
        "    print(f\"  Min:    {subset.min():.2f}%\")\n",
        "    print(f\"  Max:    {subset.max():.2f}%\")\n",
        "    print(f\"  Mean:   {subset.mean():.2f}%\")\n",
        "    print(f\"  Median: {np.median(subset):.2f}%\")\n",
        "    print(f\"  Std:    {subset.std():.2f}%\")\n",
        "\n",
        "max_count = grouped['Image Count'].max()\n",
        "min_count = grouped['Image Count'].min()\n",
        "\n",
        "print(\"\\nSubclass with most images:\")\n",
        "print(f\"  {max_count} images\")\n",
        "\n",
        "print(\"Subclass with fewest images:\")\n",
        "print(f\"  {min_count} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-ExWseSIOzC",
        "outputId": "b6ea1112-bd73-452d-f26f-3d5456b3720e"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training images: 6288\n",
            "Unique superclasses: 3\n",
            "Unique subclasses: 87\n",
            "\n",
            "Images per Superclass:\n",
            "  Superclass  Image Count  Percent of Total\n",
            "0    reptile         2354             37.44\n",
            "1        dog         2084             33.14\n",
            "2       bird         1850             29.42\n",
            "\n",
            " Stats on % of Total Dataset per (superclass, subclass):\n",
            "  Min:    0.78%\n",
            "  Max:    1.62%\n",
            "  Mean:   1.15%\n",
            "  Median: 0.80%\n",
            "  Std:    0.40%\n",
            "\n",
            " Stats on % of Superclass per subclass:\n",
            "  Min:    2.08%\n",
            "  Max:    5.41%\n",
            "  Mean:   3.45%\n",
            "  Median: 2.70%\n",
            "  Std:    1.16%\n",
            "\n",
            " Stats for subclasses within superclass: bird\n",
            "  Number of subclasses: 29\n",
            "  Min:    2.70%\n",
            "  Max:    5.41%\n",
            "  Mean:   3.45%\n",
            "  Median: 2.70%\n",
            "  Std:    1.21%\n",
            "\n",
            " Stats for subclasses within superclass: dog\n",
            "  Number of subclasses: 29\n",
            "  Min:    2.35%\n",
            "  Max:    4.80%\n",
            "  Mean:   3.45%\n",
            "  Median: 2.35%\n",
            "  Std:    1.22%\n",
            "\n",
            " Stats for subclasses within superclass: reptile\n",
            "  Number of subclasses: 29\n",
            "  Min:    2.08%\n",
            "  Max:    4.33%\n",
            "  Mean:   3.45%\n",
            "  Median: 4.25%\n",
            "  Std:    1.06%\n",
            "\n",
            "Subclass with most images:\n",
            "  102 images\n",
            "Subclass with fewest images:\n",
            "  49 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "caKLk2pKqqoN",
        "outputId": "97dd9346-689f-4334-a6f2-5bd4eda03a17"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   image  superclass_index  subclass_index  \\\n",
              "0  0.jpg                 1              37   \n",
              "1  1.jpg                 0              42   \n",
              "2  2.jpg                 1              62   \n",
              "3  3.jpg                 1              31   \n",
              "4  4.jpg                 0               4   \n",
              "\n",
              "                                         description superclass_name  \\\n",
              "0  nature photograph of a dog, specifically a Mal...             dog   \n",
              "1  nature photograph of a bird, specifically a oy...            bird   \n",
              "2  nature photograph of a dog, specifically a Afg...             dog   \n",
              "3  nature photograph of a dog, specifically a Shi...             dog   \n",
              "4  nature photograph of a bird, specifically a gr...            bird   \n",
              "\n",
              "                                    subclass_name  \n",
              "0           Maltese dog, Maltese terrier, Maltese  \n",
              "1                   oystercatcher, oyster catcher  \n",
              "2                            Afghan hound, Afghan  \n",
              "3                                        Shih-Tzu  \n",
              "4  great grey owl, great gray owl, Strix nebulosa  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb26177e-81fb-44f7-9a7b-8231a21f85ca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>superclass_index</th>\n",
              "      <th>subclass_index</th>\n",
              "      <th>description</th>\n",
              "      <th>superclass_name</th>\n",
              "      <th>subclass_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>37</td>\n",
              "      <td>nature photograph of a dog, specifically a Mal...</td>\n",
              "      <td>dog</td>\n",
              "      <td>Maltese dog, Maltese terrier, Maltese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>nature photograph of a bird, specifically a oy...</td>\n",
              "      <td>bird</td>\n",
              "      <td>oystercatcher, oyster catcher</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>62</td>\n",
              "      <td>nature photograph of a dog, specifically a Afg...</td>\n",
              "      <td>dog</td>\n",
              "      <td>Afghan hound, Afghan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>nature photograph of a dog, specifically a Shi...</td>\n",
              "      <td>dog</td>\n",
              "      <td>Shih-Tzu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>nature photograph of a bird, specifically a gr...</td>\n",
              "      <td>bird</td>\n",
              "      <td>great grey owl, great gray owl, Strix nebulosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb26177e-81fb-44f7-9a7b-8231a21f85ca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cb26177e-81fb-44f7-9a7b-8231a21f85ca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cb26177e-81fb-44f7-9a7b-8231a21f85ca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8e6c1578-cd61-45dd-9c4e-a53d0f8668a9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8e6c1578-cd61-45dd-9c4e-a53d0f8668a9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8e6c1578-cd61-45dd-9c4e-a53d0f8668a9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 6288,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6288,\n        \"samples\": [\n          \"4441.jpg\",\n          \"3658.jpg\",\n          \"6068.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"superclass_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subclass_index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24,\n        \"min\": 0,\n        \"max\": 86,\n        \"num_unique_values\": 87,\n        \"samples\": [\n          12,\n          37,\n          22\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"description\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 87,\n        \"samples\": [\n          \"nature photograph of a dog, specifically a Tibetan terrier, chrysanthemum dog, centered composition, photorealistic, 8k, the animal is fully visible within frame and not cut off\",\n          \"nature photograph of a dog, specifically a Maltese dog, Maltese terrier, Maltese, centered composition, photorealistic, 8k, the animal is fully visible within frame and not cut off\",\n          \"nature photograph of a dog, specifically a Japanese spaniel, centered composition, photorealistic, 8k, the animal is fully visible within frame and not cut off\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"superclass_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"dog\",\n          \"bird\",\n          \"reptile\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subclass_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 87,\n        \"samples\": [\n          \"Tibetan terrier, chrysanthemum dog\",\n          \"Maltese dog, Maltese terrier, Maltese\",\n          \"Japanese spaniel\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "mkhF8dh6sUq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "class MultiClassImageDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['image'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        superclass_label = row['superclass_index']\n",
        "        subclass_label = row['subclass_index']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, superclass_label, subclass_label\n",
        "\n",
        "class MultiClassImageTestDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len([fname for fname in os.listdir(self.img_dir)])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = str(idx) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name"
      ],
      "metadata": {
        "id": "QPlhfGcysWsb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Incorporate Tiny ImageNet for example images of novel data"
      ],
      "metadata": {
        "id": "RtJilJ1g5OgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "!pip install -U datasets\n",
        "ds_train = load_dataset(\"slegroux/tiny-imagenet-200-clean\", split=\"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D35f2Lik6Acc",
        "outputId": "d0854234-727a-43f2-e67f-f44d78d42f28"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip tiny-imagenet-200.zip tiny-imagenet-200/words.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH_Edthr9meP",
        "outputId": "7815ef4f-a063-42ac-fd1f-e390b25d882f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2025-05-13 18:42:03--  https://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip.1’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  57.9MB/s    in 4.6s    \n",
            "\n",
            "2025-05-13 18:42:08 (51.2 MB/s) - ‘tiny-imagenet-200.zip.1’ saved [248100043/248100043]\n",
            "\n",
            "Archive:  tiny-imagenet-200.zip\n",
            "replace tiny-imagenet-200/words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grab mapping of id to human readable label\n",
        "wnid_to_label = {}\n",
        "with open(\"tiny-imagenet-200/words.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        wnid, label = line.strip().split(\"\\t\")\n",
        "        wnid_to_label[wnid] = label\n",
        "\n",
        "# get the ids\n",
        "synsets = ds_train.features[\"label\"].names\n",
        "\n",
        "# map id to human readable labels\n",
        "idx_to_label = {i: wnid_to_label[wnid] for i, wnid in enumerate(synsets)}\n",
        "unique_labels = set(label for id, label in idx_to_label.items())\n",
        "\n",
        "print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "print(unique_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHPp0Iqx_NM9",
        "outputId": "7c69246c-b9d1-4d26-d25e-efe48c73f677"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique labels: 200\n",
            "{'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'brown bear, bruin, Ursus arctos', 'oboe, hautboy, hautbois', 'military uniform', 'brain coral', \"potter's wheel\", 'lifeboat', 'steel arch bridge', 'triumphal arch', 'vestment', 'kimono', 'suspension bridge', 'seashore, coast, seacoast, sea-coast', 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus', 'slug', 'cockroach, roach', 'mashed potato', 'fur coat', 'bathtub, bathing tub, bath, tub', 'trilobite', 'frying pan, frypan, skillet', 'gasmask, respirator, gas helmet', 'lawn mower, mower', 'altar', 'sandal', 'confectionery, confectionary, candy store', 'scoreboard', 'banana', 'goldfish, Carassius auratus', \"spider web, spider's web\", 'bison', 'organ, pipe organ', 'alp', 'grasshopper, hopper', 'maypole', 'barn', 'American alligator, Alligator mississipiensis', 'albatross, mollymawk', 'apron', 'obelisk', 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui', 'pretzel', 'meat loaf, meatloaf', 'walking stick, walkingstick, stick insect', 'cauliflower', 'picket fence, paling', 'pizza, pizza pie', 'backpack, back pack, knapsack, packsack, rucksack, haversack', 'CD player', 'bee', 'trolleybus, trolley coach, trackless trolley', 'beacon, lighthouse, beacon light, pharos', 'beaker', 'Arabian camel, dromedary, Camelus dromedarius', 'viaduct', 'sombrero', 'coral reef', 'chimpanzee, chimp, Pan troglodytes', 'miniskirt, mini', 'computer keyboard, keypad', 'lampshade, lamp shade', 'water jug', 'turnstile', 'tarantula', 'sunglasses, dark glasses, shades', 'hourglass', 'orangutan, orang, orangutang, Pongo pygmaeus', 'ice lolly, lolly, lollipop, popsicle', 'pill bottle', 'comic book', 'cardigan', 'space heater', 'plate', 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon', 'lemon', 'sea cucumber, holothurian', 'rocking chair, rocker', 'bell pepper', 'Labrador retriever', 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle', 'pole', 'freight car', 'cannon', 'tractor', 'sports car, sport car', 'volleyball', 'hog, pig, grunter, squealer, Sus scrofa', 'Chihuahua', 'rugby ball', 'baboon', 'lion, king of beasts, Panthera leo', 'flagpole, flagstaff', 'guacamole', 'goose', 'pay-phone, pay-station', 'broom', 'butcher shop, meat market', 'crane', 'parking meter', 'sewing machine', 'punching bag, punch bag, punching ball, punchball', 'European fire salamander, Salamandra salamandra', 'limousine, limo', 'Yorkshire terrier', 'Egyptian cat', 'iPod', 'potpie', 'lakeside, lakeshore', 'refrigerator, icebox', 'drumstick', 'basketball', 'scorpion', 'bullfrog, Rana catesbeiana', 'tabby, tabby cat', 'bullet train, bullet', 'snorkel', 'gazelle', 'cliff, drop, drop-off', 'centipede', 'guinea pig, Cavia cobaya', 'snail', 'jinrikisha, ricksha, rickshaw', 'remote control, remote', 'mantis, mantid', 'sulphur butterfly, sulfur butterfly', 'teddy, teddy bear', 'black widow, Latrodectus mactans', 'syringe', 'dam, dike, dyke', 'chest', 'magnetic compass', 'birdhouse', 'jellyfish', 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis', 'ice cream, icecream', 'neck brace', 'orange', 'barrel, cask', 'sock', 'chain', 'German shepherd, German shepherd dog, German police dog, alsatian', 'swimming trunks, bathing trunks', 'water tower', 'mushroom', 'go-kart', 'nail', 'bannister, banister, balustrade, balusters, handrail', 'projectile, missile', 'thatch, thatched roof', 'bucket, pail', 'wooden spoon', 'black stork, Ciconia nigra', 'ox', 'abacus', 'wok', 'acorn', 'torch', 'school bus', 'cliff dwelling', 'Christmas stocking', 'moving van', 'dining table, board', 'bikini, two-piece', 'binoculars, field glasses, opera glasses', 'Persian cat', 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus', 'desk', 'stopwatch, stop watch', 'umbrella', 'candle, taper, wax light', \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\", 'dumbbell', 'gondola', 'reel', 'king penguin, Aptenodytes patagonica', 'pop bottle, soda bottle', 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish', \"academic gown, academic robe, judge's robe\", \"plunger, plumber's helper\", 'boa constrictor, Constrictor constrictor', 'golden retriever', 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM', 'convertible', 'dugong, Dugong dugon', 'pomegranate', 'sea slug, nudibranch', 'fly', 'barbershop', 'bow tie, bow-tie, bowtie', 'poncho', 'African elephant, Loxodonta africana', 'espresso', 'brass, memorial tablet, plaque', 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria', 'teapot', 'American lobster, Northern lobster, Maine lobster, Homarus americanus', 'standard poodle', 'beer bottle', 'fountain', 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manually extracted novel classes from dataset\n",
        "non_reptile_bird_dog_labels = {\n",
        "    'thatch, thatched roof',\n",
        "    'parking meter',\n",
        "    'syringe',\n",
        "    'gondola',\n",
        "    'dumbbell',\n",
        "    'altar',\n",
        "    'drumstick',\n",
        "    'centipede',\n",
        "    'cannon',\n",
        "    'limousine, limo',\n",
        "    'stopwatch, stop watch',\n",
        "    'CD player',\n",
        "    'basketball',\n",
        "    'meat loaf, meatloaf',\n",
        "    'chain',\n",
        "    'orangutan, orang, orangutang, Pongo pygmaeus',\n",
        "    'brass, memorial tablet, plaque',\n",
        "    'sunglasses, dark glasses, shades',\n",
        "    'walking stick, walkingstick, stick insect',\n",
        "    'sulphur butterfly, sulfur butterfly',\n",
        "    'sea slug, nudibranch',\n",
        "    'comic book',\n",
        "    'bell pepper',\n",
        "    'pomegranate',\n",
        "    'convertible',\n",
        "    'triumphal arch',\n",
        "    'punching bag, punch bag, punching ball, punchball',\n",
        "    \"spider web, spider's web\",\n",
        "    'miniskirt, mini',\n",
        "    'mushroom',\n",
        "    'frying pan, frypan, skillet',\n",
        "    'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
        "    'bikini, two-piece',\n",
        "    'cockroach, roach',\n",
        "    'sewing machine',\n",
        "    'cliff, drop, drop-off',\n",
        "    'orange',\n",
        "    'military uniform',\n",
        "    'refrigerator, icebox',\n",
        "    'beer bottle',\n",
        "    'cauliflower',\n",
        "    'slug',\n",
        "    'scoreboard',\n",
        "    'poncho',\n",
        "    'desk',\n",
        "    'guacamole',\n",
        "    'bison',\n",
        "    'rocking chair, rocker',\n",
        "    'Christmas stocking',\n",
        "    'espresso',\n",
        "    'obelisk',\n",
        "    'tarantula',\n",
        "    'candle, taper, wax light',\n",
        "    'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
        "    'jellyfish',\n",
        "    'vestment',\n",
        "    'water tower',\n",
        "    'pop bottle, soda bottle',\n",
        "    'chimpanzee, chimp, Pan troglodytes',\n",
        "    'wok',\n",
        "    'pretzel',\n",
        "    'lampshade, lamp shade',\n",
        "    'turnstile',\n",
        "    'potpie',\n",
        "    'go-kart',\n",
        "    'suspension bridge',\n",
        "    'computer keyboard, keypad',\n",
        "    'snorkel',\n",
        "    'barbershop',\n",
        "    'banana',\n",
        "    'water jug',\n",
        "    'hourglass',\n",
        "    'ice lolly, lolly, lollipop, popsicle',\n",
        "    'cliff dwelling',\n",
        "    'hog, pig, grunter, squealer, Sus scrofa',\n",
        "    'sea cucumber, holothurian',\n",
        "    'king penguin, Aptenodytes patagonica',\n",
        "    'abacus',\n",
        "    'tabby, tabby cat',\n",
        "    'dining table, board',\n",
        "    'wooden spoon',\n",
        "    'acorn',\n",
        "    'bow tie, bow-tie, bowtie',\n",
        "    'pay-phone, pay-station',\n",
        "    'lion, king of beasts, Panthera leo',\n",
        "    'gazelle',\n",
        "    'chest',\n",
        "    'beaker',\n",
        "    'lawn mower, mower',\n",
        "    'confectionery, confectionary, candy store',\n",
        "    'lemon',\n",
        "    'pole',\n",
        "    'steel arch bridge',\n",
        "    'bullet train, bullet',\n",
        "    'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
        "    'mashed potato',\n",
        "    'alp',\n",
        "    'guinea pig, Cavia cobaya',\n",
        "    'trilobite',\n",
        "    'bathtub, bathing tub, bath, tub',\n",
        "    'tractor',\n",
        "    'sock',\n",
        "    'bucket, pail',\n",
        "    'jinrikisha, ricksha, rickshaw',\n",
        "    'kimono',\n",
        "    'binoculars, field glasses, opera glasses',\n",
        "    'pill bottle',\n",
        "    'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
        "    'snail',\n",
        "    'dugong, Dugong dugon',\n",
        "    'pizza, pizza pie',\n",
        "    'barrel, cask',\n",
        "    'brown bear, bruin, Ursus arctos',\n",
        "    'viaduct',\n",
        "    'gasmask, respirator, gas helmet',\n",
        "    \"plunger, plumber's helper\",\n",
        "    'moving van',\n",
        "    'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
        "    'broom',\n",
        "    'swimming trunks, bathing trunks',\n",
        "    'projectile, missile',\n",
        "    'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
        "    'brain coral',\n",
        "    'freight car',\n",
        "    'sports car, sport car',\n",
        "    'dam, dike, dyke',\n",
        "    'remote control, remote',\n",
        "    'sandal',\n",
        "    'school bus',\n",
        "    'fountain',\n",
        "    'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
        "    'mantis, mantid',\n",
        "    'scorpion',\n",
        "    'volleyball',\n",
        "    'nail',\n",
        "    'trolleybus, trolley coach, trackless trolley',\n",
        "    'Persian cat',\n",
        "    'lifeboat',\n",
        "    'teapot',\n",
        "    'crane',\n",
        "    'umbrella',\n",
        "    'lakeside, lakeshore',\n",
        "    'barn',\n",
        "    'organ, pipe organ',\n",
        "    'ice cream, icecream',\n",
        "    'Arabian camel, dromedary, Camelus dromedarius',\n",
        "    'oboe, hautboy, hautbois',\n",
        "    'reel',\n",
        "    'apron',\n",
        "    'beacon, lighthouse, beacon light, pharos',\n",
        "    'sombrero',\n",
        "    'flagpole, flagstaff',\n",
        "    'Egyptian cat',\n",
        "    'torch',\n",
        "    'bee',\n",
        "    'butcher shop, meat market',\n",
        "    'plate',\n",
        "    'fly',\n",
        "    'cardigan',\n",
        "    'ox',\n",
        "    \"potter's wheel\",\n",
        "    'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
        "    'seashore, coast, seacoast, sea-coast',\n",
        "    'rugby ball',\n",
        "    'iPod',\n",
        "    'African elephant, Loxodonta africana',\n",
        "    'teddy, teddy bear',\n",
        "    \"academic gown, academic robe, judge's robe\",\n",
        "    'birdhouse',\n",
        "    'bannister, banister, balustrade, balusters, handrail',\n",
        "    'magnetic compass',\n",
        "    'grasshopper, hopper',\n",
        "    'maypole',\n",
        "    'picket fence, paling',\n",
        "    'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
        "    'fur coat',\n",
        "    'neck brace',\n",
        "    'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
        "    'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
        "    'black widow, Latrodectus mactans'\n",
        "}\n",
        "\n",
        "filtered_data = [data for data in ds_train if idx_to_label[data['label']] in non_reptile_bird_dog_labels]\n",
        "print(len(filtered_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRutzaTr5WTr",
        "outputId": "53f0c07e-466e-4e19-cc16-e6e6c8352461"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "num_samples = 2000\n",
        "sampled_filtered_ds = ds_train.shuffle(seed=42).select(range(num_samples))\n",
        "label_counts = Counter([data['label'] for data in sampled_filtered_ds])\n",
        "counts = list(label_counts.values())\n",
        "\n",
        "print(f\"Total labels: {sum(counts)}\")\n",
        "print(f\"Unique labels: {len(label_counts)}\")\n",
        "print(f\"Max samples per label: {max(counts)}\")\n",
        "print(f\"Min samples per label: {min(counts)}\")\n",
        "print(f\"Average samples per label: {sum(counts) / len(counts):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMJ-FOig9FB6",
        "outputId": "1f7df9a1-0db2-45f3-b210-f1e180d68208"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total labels: 2000\n",
            "Unique labels: 200\n",
            "Max samples per label: 18\n",
            "Min samples per label: 3\n",
            "Average samples per label: 10.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# move these novel samples to train_image folder\n",
        "oe_image_dir = base_dir + \"data/train_images/\"\n",
        "\n",
        "oe_rows = []\n",
        "for i, item in tqdm(enumerate(sampled_filtered_ds), total=len(sampled_filtered_ds)):\n",
        "    image = item[\"image\"]\n",
        "    filename = f\"novel_{i}.jpg\"\n",
        "    image_path = os.path.join(oe_image_dir, filename)\n",
        "    image.save(image_path)\n",
        "\n",
        "    oe_rows.append({\n",
        "        \"image\": filename,\n",
        "        \"superclass_index\": 3,\n",
        "        \"subclass_index\": 87,\n",
        "        \"superclass_name\": \"novel\",\n",
        "        \"subclass_name\": \"novel\"\n",
        "    })\n",
        "\n",
        "novel_df = pd.DataFrame(oe_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-ktxnezH-zg",
        "outputId": "ec370443-6f72-4930-8c3d-04162096e144"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [01:12<00:00, 27.47it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utlity Helper Methods"
      ],
      "metadata": {
        "id": "Wuwyvdc067x6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_prediction_distributions(prediction_distributions):\n",
        "    # === Confidence Stats ===\n",
        "    for key in [\"super_confidence\", \"sub_confidence\"]:\n",
        "        values = np.array(prediction_distributions[key])\n",
        "        print(f\"\\n Stats for {key.replace('_', ' ').title()}:\")\n",
        "        print(f\"  Count:     {len(values)}\")\n",
        "        print(f\"  Mean:      {values.mean():.4f}\")\n",
        "        print(f\"  Std Dev:   {values.std():.4f}\")\n",
        "        print(f\"  Min:       {values.min():.4f}\")\n",
        "        print(f\"  Max:       {values.max():.4f}\")\n",
        "        print(f\"  25th pct:  {np.percentile(values, 25):.4f}\")\n",
        "        print(f\"  Median:    {np.median(values):.4f}\")\n",
        "        print(f\"  75th pct:  {np.percentile(values, 75):.4f}\")\n",
        "        below_50 = (values < 0.5).sum()\n",
        "        print(f\"  Below 0.5: {below_50} samples ({below_50 / len(values) * 100:.2f}%)\")\n",
        "\n",
        "    # === Prediction Counts ===\n",
        "    print(\"\\n Raw Superclass Prediction Distribution:\")\n",
        "    super_counts = Counter(prediction_distributions[\"raw_superclass_pred\"])\n",
        "    for label, count in sorted(super_counts.items()):\n",
        "        print(f\"  Superclass {label}: {count} samples\")\n",
        "\n",
        "    print(\"\\n Raw Subclass Prediction Distribution (Top 15):\")\n",
        "    subclass_counts = Counter(prediction_distributions[\"raw_subclass_pred\"])\n",
        "    for label, count in subclass_counts.most_common(15):\n",
        "        print(f\"  Subclass {label}: {count} samples\")\n",
        "\n",
        "\n",
        "    print(\"\\n Raw Subclass Prediction Distribution (Bottom 15):\")\n",
        "    subclass_counts = Counter(prediction_distributions[\"raw_subclass_pred\"])\n",
        "    for label, count in sorted(subclass_counts.items(), key=lambda x: x[1])[:15]:\n",
        "        print(f\"  Subclass {label}: {count} samples\")"
      ],
      "metadata": {
        "id": "7sQNvzlNCBwK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# visualize the predictions\n",
        "def print_confidence_stats(confidences, label):\n",
        "    confidences = np.array(confidences)\n",
        "    print(f\"\\n Stats for {label} confidence:\")\n",
        "    print(f\"  Mean:      {confidences.mean():.4f}\")\n",
        "    print(f\"  Std Dev:   {confidences.std():.4f}\")\n",
        "    print(f\"  Min:       {confidences.min():.4f}\")\n",
        "    print(f\"  Max:       {confidences.max():.4f}\")\n",
        "    print(f\"  25th pct:  {np.percentile(confidences, 25):.4f}\")\n",
        "    print(f\"  50th pct:  {np.percentile(confidences, 50):.4f} (median)\")\n",
        "    print(f\"  75th pct:  {np.percentile(confidences, 75):.4f}\")\n",
        "    print(f\"  Below 0.5: {(confidences < 0.5).sum()} samples ({(confidences < 0.5).mean()*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "jb-QaBNLH3i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(transform, use_novel_data=True):\n",
        "    \"\"\"\n",
        "    Given a transform and whether to use novel data in training,\n",
        "    returns training and validation datasets\n",
        "    \"\"\"\n",
        "    training_data = train_df if not use_novel_data else pd.concat([train_df, novel_df], ignore_index=True)\n",
        "\n",
        "    full_dataset = MultiClassImageDataset(training_data, img_dir=image_dir, transform=transform)\n",
        "\n",
        "    train_size = int(0.9 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_dataset, [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "QGsqzBtTJrQ0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import RandAugment\n",
        "def get_transform(data_augmentation=False, model=\"Resnet50\"):\n",
        "    if model == \"Resnet50\":\n",
        "        base_transforms = [\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                                 std=(0.229, 0.224, 0.225))\n",
        "        ]\n",
        "\n",
        "        if data_augmentation:\n",
        "            aug_transforms = [\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(degrees=15),\n",
        "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "                transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0))\n",
        "                # RandAugment(num_ops=2, magnitude=9),\n",
        "            ]\n",
        "            return transforms.Compose(aug_transforms + base_transforms)\n",
        "        else:\n",
        "            return transforms.Compose(base_transforms)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"No transform configured for model '{model}'\")\n"
      ],
      "metadata": {
        "id": "jkfb_zL39KN3"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train ResNet50"
      ],
      "metadata": {
        "id": "smE37qNUxOIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineClassifier(nn.Module):\n",
        "    def __init__(self, in_features, num_classes, scale=10.0):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(num_classes, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = nn.functional.normalize(x, dim=1)\n",
        "        w_norm = nn.functional.normalize(self.weight, dim=1)\n",
        "        return self.scale * torch.matmul(x_norm, w_norm.T)"
      ],
      "metadata": {
        "id": "nC3CdIzW8IqP"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class ResNet50MultiHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_superclasses=4,\n",
        "        num_subclasses=88,\n",
        "        use_dropout=False,\n",
        "        dropout_p=0.5,\n",
        "        use_nonlinear_head=False,\n",
        "        use_cosine_classifier=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        ResNet-50 backbone with dual heads. Each head can be:\n",
        "          - Linear (default), or\n",
        "          - Non-linear MLP: Linear -> ReLU -> Dropout -> Linear\n",
        "\n",
        "        Args:\n",
        "            num_superclasses (int): Number of superclass categories.\n",
        "            num_subclasses (int): Number of subclass categories.\n",
        "            use_dropout (bool): Whether to apply dropout in head.\n",
        "            dropout_p (float): Dropout probability.\n",
        "            use_nonlinear_head (bool): If True, use 2-layer MLP head.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(pretrained=True)\n",
        "\n",
        "        # Freeze most layers except last block\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if \"layer4\" in name or \"avgpool\" in name:\n",
        "                param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.embedding_dim = 2048\n",
        "        self.use_dropout = use_dropout\n",
        "        self.use_nonlinear_head = use_nonlinear_head\n",
        "        self.use_cosine_classifier = use_cosine_classifier\n",
        "\n",
        "        # === Superclass Head ===\n",
        "        if self.use_cosine_classifier:\n",
        "            self.superclass_head = CosineClassifier(self.embedding_dim, num_superclasses)\n",
        "        elif self.use_nonlinear_head:\n",
        "            self.superclass_head = nn.Sequential(\n",
        "                nn.Linear(self.embedding_dim, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=dropout_p) if use_dropout else nn.Identity(),\n",
        "                nn.Linear(512, num_superclasses)\n",
        "            )\n",
        "        else:\n",
        "            self.superclass_head = nn.Linear(self.embedding_dim, num_superclasses)\n",
        "\n",
        "        # === Subclass Head ===\n",
        "        if self.use_cosine_classifier:\n",
        "            self.subclass_head = CosineClassifier(self.embedding_dim, num_subclasses)\n",
        "        elif self.use_nonlinear_head:\n",
        "            self.subclass_head = nn.Sequential(\n",
        "                nn.Linear(self.embedding_dim, 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(p=dropout_p) if use_dropout else nn.Identity(),\n",
        "                nn.Linear(512, num_subclasses)\n",
        "            )\n",
        "        else:\n",
        "            self.subclass_head = nn.Linear(self.embedding_dim, num_subclasses)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        super_logits = self.superclass_head(features)\n",
        "        sub_logits = self.subclass_head(features)\n",
        "        return super_logits, sub_logits"
      ],
      "metadata": {
        "id": "vKj8We96xP14"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # === Prediction tracking for analysis ===\n",
        "resnet_training_prediction_distributions = {\n",
        "  \"raw_superclass_pred\": [],\n",
        "  \"raw_subclass_pred\": [],\n",
        "    \"super_confidence\": [],\n",
        "    \"sub_confidence\": []\n",
        "  }\n",
        "\n",
        "\n",
        "# Code block to train Resnet\n",
        "# === Code block to train ResNet-50 ===\n",
        "def train_resnet50(use_novel_data=True, use_nonlinear_head=False, use_augmentation=False, use_cosine_classifier=False, use_dropout=False, dropout_p=0.5, num_epochs=20):\n",
        "    \"\"\"\n",
        "    Trains a ResNet-50 model with two heads (superclass and subclass).\n",
        "\n",
        "    Args:\n",
        "        use_novel_data (bool): Whether to include novel data during training.\n",
        "        use_dropout (bool): Whether to use dropout before classification heads.\n",
        "        dropout_p (float): Dropout probability.\n",
        "        num_epochs (int): Number of training epochs.\n",
        "\n",
        "    Returns:\n",
        "        Trained model (ResNet50MultiHead)\n",
        "    \"\"\"\n",
        "    # === Image transforms ===\n",
        "    image_transforms = get_transform(data_augmentation=use_augmentation, model=\"Resnet50\")\n",
        "\n",
        "    # === Load dataset ===\n",
        "    train_dataset, val_dataset = get_dataset(transform=image_transforms, use_novel_data=use_novel_data)\n",
        "    batch_size = 64\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    # === Model, Loss, Optimizer Setup ===\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    model = ResNet50MultiHead(use_nonlinear_head=use_nonlinear_head, use_cosine_classifier=use_cosine_classifier, use_dropout=use_dropout, dropout_p=dropout_p).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # === Training loop ===\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, super_labels, sub_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            images = images.to(device)\n",
        "            super_labels = super_labels.to(device)\n",
        "            sub_labels = sub_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            super_logits, sub_logits = model(images)\n",
        "\n",
        "            super_loss = criterion(super_logits, super_labels)\n",
        "            sub_loss = criterion(sub_logits, sub_labels)\n",
        "            loss = super_loss + sub_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # === Validation ===\n",
        "        model.eval()\n",
        "        super_correct = sub_correct = total = 0\n",
        "        super_loss_total, sub_loss_total = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, super_labels, sub_labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                super_labels = super_labels.to(device)\n",
        "                sub_labels = sub_labels.to(device)\n",
        "\n",
        "                super_logits, sub_logits = model(images)\n",
        "\n",
        "                # === Compute and accumulate validation loss\n",
        "                super_loss = criterion(super_logits, super_labels)\n",
        "                sub_loss = criterion(sub_logits, sub_labels)\n",
        "                super_loss_total += super_loss.item()\n",
        "                sub_loss_total += sub_loss.item()\n",
        "\n",
        "                super_preds = torch.argmax(super_logits, dim=1)\n",
        "                sub_preds = torch.argmax(sub_logits, dim=1)\n",
        "\n",
        "                super_correct += (super_preds == super_labels).sum().item()\n",
        "                sub_correct += (sub_preds == sub_labels).sum().item()\n",
        "                total += images.size(0)\n",
        "\n",
        "                # Confidence tracking\n",
        "                super_probs = torch.softmax(super_logits, dim=1)\n",
        "                sub_probs = torch.softmax(sub_logits, dim=1)\n",
        "                super_conf, _ = torch.max(super_probs, dim=1)\n",
        "                sub_conf, _ = torch.max(sub_probs, dim=1)\n",
        "\n",
        "                resnet_training_prediction_distributions[\"raw_superclass_pred\"].extend(super_preds.cpu().tolist())\n",
        "                resnet_training_prediction_distributions[\"raw_subclass_pred\"].extend(sub_preds.cpu().tolist())\n",
        "                resnet_training_prediction_distributions[\"super_confidence\"].extend(super_conf.cpu().tolist())\n",
        "                resnet_training_prediction_distributions[\"sub_confidence\"].extend(sub_conf.cpu().tolist())\n",
        "\n",
        "        avg_super_loss = super_loss_total / len(val_loader)\n",
        "        avg_sub_loss = sub_loss_total / len(val_loader)\n",
        "\n",
        "        print(f\"Validation Accuracy | Superclass: {super_correct / total:.4f} | Subclass: {sub_correct / total:.4f}\")\n",
        "        print(f\"Validation Loss     | Superclass: {avg_super_loss:.4f} | Subclass: {avg_sub_loss:.4f}\")\n",
        "\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "90deByTI-hAJ"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_resnet50(\n",
        "    use_novel_data=False,\n",
        "    use_augmentation=False,\n",
        "    use_nonlinear_head=False,\n",
        "    use_dropout=False,\n",
        "    dropout_p=0.5,\n",
        "    use_cosine_classifier=False,\n",
        "    num_epochs=20\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JLm0sux_NrC",
        "outputId": "9457759a-2405-4508-d9c0-6d3daca7c4d3"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1: 100%|██████████| 89/89 [00:22<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 4.8008\n",
            "Validation Accuracy | Superclass: 0.9857 | Subclass: 0.4467\n",
            "Validation Loss     | Superclass: 0.4566 | Subclass: 3.5122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 89/89 [00:22<00:00,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 | Train Loss: 3.5006\n",
            "Validation Accuracy | Superclass: 0.9936 | Subclass: 0.6216\n",
            "Validation Loss     | Superclass: 0.2522 | Subclass: 2.7408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 89/89 [00:22<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 | Train Loss: 2.6864\n",
            "Validation Accuracy | Superclass: 0.9936 | Subclass: 0.7472\n",
            "Validation Loss     | Superclass: 0.1686 | Subclass: 2.1549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 89/89 [00:22<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 | Train Loss: 2.1325\n",
            "Validation Accuracy | Superclass: 0.9936 | Subclass: 0.7742\n",
            "Validation Loss     | Superclass: 0.1267 | Subclass: 1.7758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 89/89 [00:22<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 | Train Loss: 1.7412\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8172\n",
            "Validation Loss     | Superclass: 0.1027 | Subclass: 1.4808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 89/89 [00:21<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 | Train Loss: 1.4712\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8458\n",
            "Validation Loss     | Superclass: 0.0823 | Subclass: 1.2509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 89/89 [00:22<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 | Train Loss: 1.2592\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8633\n",
            "Validation Loss     | Superclass: 0.0717 | Subclass: 1.1131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 89/89 [00:22<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 | Train Loss: 1.1064\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8601\n",
            "Validation Loss     | Superclass: 0.0625 | Subclass: 1.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 89/89 [00:21<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 | Train Loss: 0.9853\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8696\n",
            "Validation Loss     | Superclass: 0.0555 | Subclass: 0.9093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 89/89 [00:22<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 | Train Loss: 0.8972\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8680\n",
            "Validation Loss     | Superclass: 0.0501 | Subclass: 0.8421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 89/89 [00:22<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 | Train Loss: 0.8078\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8744\n",
            "Validation Loss     | Superclass: 0.0449 | Subclass: 0.7717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 89/89 [00:22<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 | Train Loss: 0.7452\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8919\n",
            "Validation Loss     | Superclass: 0.0424 | Subclass: 0.7177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 89/89 [00:22<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 | Train Loss: 0.6864\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8903\n",
            "Validation Loss     | Superclass: 0.0393 | Subclass: 0.6843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 89/89 [00:22<00:00,  3.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 | Train Loss: 0.6410\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8967\n",
            "Validation Loss     | Superclass: 0.0364 | Subclass: 0.6438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 89/89 [00:22<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 | Train Loss: 0.6005\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8871\n",
            "Validation Loss     | Superclass: 0.0346 | Subclass: 0.6067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 89/89 [00:22<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 | Train Loss: 0.5666\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8998\n",
            "Validation Loss     | Superclass: 0.0315 | Subclass: 0.5778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 89/89 [00:22<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 | Train Loss: 0.5301\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.9030\n",
            "Validation Loss     | Superclass: 0.0309 | Subclass: 0.5623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 89/89 [00:22<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 | Train Loss: 0.5040\n",
            "Validation Accuracy | Superclass: 0.9968 | Subclass: 0.9062\n",
            "Validation Loss     | Superclass: 0.0305 | Subclass: 0.5330\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 89/89 [00:22<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 | Train Loss: 0.4807\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.8935\n",
            "Validation Loss     | Superclass: 0.0286 | Subclass: 0.5200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████| 89/89 [00:22<00:00,  3.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 | Train Loss: 0.4598\n",
            "Validation Accuracy | Superclass: 0.9952 | Subclass: 0.9126\n",
            "Validation Loss     | Superclass: 0.0257 | Subclass: 0.4938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the prediction distributions\n",
        "analyze_prediction_distributions(resnet_training_prediction_distributions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zENB-wMfH9KG",
        "outputId": "c099af80-409d-4253-9f64-e8689683a505"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Stats for Super Confidence:\n",
            "  Count:     16580\n",
            "  Mean:      0.9723\n",
            "  Std Dev:   0.0336\n",
            "  Min:       0.4372\n",
            "  Max:       0.9982\n",
            "  25th pct:  0.9681\n",
            "  Median:    0.9813\n",
            "  75th pct:  0.9880\n",
            "  Below 0.5: 7 samples (0.04%)\n",
            "\n",
            " Stats for Sub Confidence:\n",
            "  Count:     16580\n",
            "  Mean:      0.8364\n",
            "  Std Dev:   0.2196\n",
            "  Min:       0.0378\n",
            "  Max:       0.9948\n",
            "  25th pct:  0.8226\n",
            "  Median:    0.9340\n",
            "  75th pct:  0.9634\n",
            "  Below 0.5: 1748 samples (10.54%)\n",
            "\n",
            " Raw Superclass Prediction Distribution:\n",
            "  Superclass 0: 4014 samples\n",
            "  Superclass 1: 4142 samples\n",
            "  Superclass 2: 4833 samples\n",
            "  Superclass 3: 3591 samples\n",
            "\n",
            " Raw Subclass Prediction Distribution (Top 15):\n",
            "  Subclass 87: 3607 samples\n",
            "  Subclass 69: 358 samples\n",
            "  Subclass 76: 319 samples\n",
            "  Subclass 21: 315 samples\n",
            "  Subclass 30: 303 samples\n",
            "  Subclass 75: 285 samples\n",
            "  Subclass 44: 281 samples\n",
            "  Subclass 6: 275 samples\n",
            "  Subclass 27: 272 samples\n",
            "  Subclass 63: 268 samples\n",
            "  Subclass 70: 250 samples\n",
            "  Subclass 37: 243 samples\n",
            "  Subclass 52: 241 samples\n",
            "  Subclass 29: 240 samples\n",
            "  Subclass 66: 229 samples\n",
            "\n",
            " Raw Subclass Prediction Distribution (Bottom 15):\n",
            "  Subclass 0: 32 samples\n",
            "  Subclass 39: 36 samples\n",
            "  Subclass 19: 40 samples\n",
            "  Subclass 73: 42 samples\n",
            "  Subclass 14: 57 samples\n",
            "  Subclass 12: 59 samples\n",
            "  Subclass 78: 60 samples\n",
            "  Subclass 25: 62 samples\n",
            "  Subclass 9: 63 samples\n",
            "  Subclass 83: 67 samples\n",
            "  Subclass 74: 68 samples\n",
            "  Subclass 45: 71 samples\n",
            "  Subclass 8: 71 samples\n",
            "  Subclass 20: 80 samples\n",
            "  Subclass 11: 83 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating CSV for Submitting to Leaderboard"
      ],
      "metadata": {
        "id": "mnoVoEGO9yG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using these to check the distributions of predictions\n",
        "test_prediction_distributions = {\n",
        "    \"raw_superclass_pred\": [],\n",
        "    \"raw_subclass_pred\": [],\n",
        "    \"super_confidence\": [],\n",
        "    \"sub_confidence\": [],\n",
        "    \"super_logit\": [],\n",
        "    \"sub_logit\": []\n",
        "}\n",
        "\n",
        "# These are better if trained on novel data\n",
        "superclass_cutoff = 0.95 # if bleow this predict novel\n",
        "subclass_cutoff = 0.6 # if below this predict novel\n",
        "\n",
        "super_logit_percentile_cutoff = 35\n",
        "sub_logit_percentile_cutoff = 50\n",
        "\n",
        "\n",
        "# These are better if trained not on novel\n",
        "# superclass_cutoff = 0.98 # if bleow this predict novel (better for when not trained with novel data)\n",
        "# subclass_cutoff = 0.75 # if below this predict novel\n",
        "\n",
        "# === Submission Function ===\n",
        "def generate_submission_csv(model, test_dir, output_path, transform, device, include_cutoffs=True):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    test_dataset = MultiClassImageTestDataset(img_dir=test_dir, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    predictions = {\n",
        "        \"image\": [],\n",
        "        \"superclass_index\": [],\n",
        "        \"subclass_index\": []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, img_names in tqdm(test_loader):\n",
        "          images = images.to(device)\n",
        "          super_logits, sub_logits = model(images)\n",
        "\n",
        "          super_probs = torch.softmax(super_logits, dim=1)\n",
        "          sub_probs = torch.softmax(sub_logits, dim=1)\n",
        "\n",
        "          super_logit_val, _ = torch.max(super_logits, dim=1)\n",
        "          sub_logit_val, _ = torch.max(sub_logits, dim=1)\n",
        "\n",
        "          super_conf, super_preds = torch.max(super_probs, dim=1)\n",
        "          sub_conf, sub_preds = torch.max(sub_probs, dim=1)\n",
        "\n",
        "          # Record raw values for analysis\n",
        "          test_prediction_distributions[\"raw_superclass_pred\"].append(super_preds.item())\n",
        "          test_prediction_distributions[\"raw_subclass_pred\"].append(sub_preds.item())\n",
        "          test_prediction_distributions[\"super_confidence\"].append(super_conf.item())\n",
        "          test_prediction_distributions[\"sub_confidence\"].append(sub_conf.item())\n",
        "          test_prediction_distributions[\"super_logit\"].append(super_logit_val.item())\n",
        "          test_prediction_distributions[\"sub_logit\"].append(sub_logit_val.item())\n",
        "\n",
        "          # === Logit Thresholds ===\n",
        "\n",
        "          # Add novel class detection threshold\n",
        "          if include_cutoffs:\n",
        "            if super_conf.item() < superclass_cutoff:\n",
        "                super_preds[0] = 3\n",
        "            if super_preds[0] == 3 or sub_conf.item() < subclass_cutoff:\n",
        "                sub_preds[0] = 87\n",
        "\n",
        "          predictions[\"image\"].append(img_names[0])\n",
        "          predictions[\"superclass_index\"].append(super_preds.item())\n",
        "          predictions[\"subclass_index\"].append(sub_preds.item())\n",
        "\n",
        "    if include_cutoffs:\n",
        "      # === Compute 25th percentile logit thresholds ===\n",
        "      super_logit_array = np.array(test_prediction_distributions[\"super_logit\"])\n",
        "      sub_logit_array = np.array(test_prediction_distributions[\"sub_logit\"])\n",
        "\n",
        "      super_logit_thresh = np.percentile(super_logit_array, super_logit_percentile_cutoff)\n",
        "      sub_logit_thresh = np.percentile(sub_logit_array, sub_logit_percentile_cutoff)\n",
        "\n",
        "      print(f\"Logit thresholds — Super: {super_logit_thresh:.4f}, Sub: {sub_logit_thresh:.4f}\")\n",
        "\n",
        "      # === Generate second CSV using raw logit thresholds ===\n",
        "      logit_predictions = {\n",
        "          \"image\": [],\n",
        "          \"superclass_index\": [],\n",
        "          \"subclass_index\": []\n",
        "      }\n",
        "\n",
        "      for i in range(len(super_logit_array)):\n",
        "          super_pred = test_prediction_distributions[\"raw_superclass_pred\"][i]\n",
        "          sub_pred = test_prediction_distributions[\"raw_subclass_pred\"][i]\n",
        "          super_logit = super_logit_array[i]\n",
        "          sub_logit = sub_logit_array[i]\n",
        "\n",
        "          if super_logit < super_logit_thresh:\n",
        "              super_pred = 3\n",
        "          if super_pred == 3 or sub_logit < sub_logit_thresh:\n",
        "              sub_pred = 87\n",
        "\n",
        "          logit_predictions[\"image\"].append(predictions[\"image\"][i])\n",
        "          logit_predictions[\"superclass_index\"].append(super_pred)\n",
        "          logit_predictions[\"subclass_index\"].append(sub_pred)\n",
        "\n",
        "      # === Save logit-based submission ===\n",
        "      logit_output_path = output_path.replace(\".csv\", \"_logit.csv\")\n",
        "      pd.DataFrame(logit_predictions).to_csv(logit_output_path, index=False)\n",
        "      print(f\"Saved logit-based submission: {logit_output_path}\")\n",
        "\n",
        "    # === Save softmax-based submission with suffix ===\n",
        "    softmax_output_path = output_path.replace(\".csv\", \"_softmax.csv\")\n",
        "    pd.DataFrame(predictions).to_csv(softmax_output_path, index=False)\n",
        "    print(f\"Saved softmax-based submission to: {softmax_output_path}\")"
      ],
      "metadata": {
        "id": "lcpljDS593kj"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "generate_submission_csv(\n",
        "    model=model,\n",
        "    test_dir=\"/content/drive/MyDrive/COMS_4995/final_project/data/test_images\",\n",
        "    output_path=\"/content/drive/MyDrive/COMS_4995/final_project/results/test_predictions.csv\",\n",
        "    transform=get_transform(data_augmentation=False, model=\"Resnet50\"), # this code is for resnet\n",
        "    # transform=preprocess, # transorm needed for clip\n",
        "    # transform=efficient_net_transforms, # transform needed for efficientnet\n",
        "    device=device,\n",
        "    include_cutoffs=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "NuoSD4RX-uo6",
        "outputId": "48b8a1dc-9f63-4470-bdc9-5b6e70bc4b69"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 112/11180 [00:01<03:07, 58.96it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-186-5d3bae21cc72>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using device: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m generate_submission_csv(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/COMS_4995/final_project/data/test_images\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-182-53d820065b46>\u001b[0m in \u001b[0;36mgenerate_submission_csv\u001b[0;34m(model, test_dir, output_path, transform, device, include_cutoffs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_names\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m           \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m           \u001b[0msuper_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-98bf26c335f1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_prediction_distributions(test_prediction_distributions)"
      ],
      "metadata": {
        "id": "hMoOx5_acl0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BgJqdt_6-t_V"
      }
    }
  ]
}